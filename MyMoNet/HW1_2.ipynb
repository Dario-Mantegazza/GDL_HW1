{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n"
          ]
        }
      ],
      "source": [
        "#import graph, coarsening, utils\n",
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport graph\n",
        "%aimport coarsening\n",
        "%aimport utils\n",
        "\n",
        "import tensorflow as tf\n",
        "import time, shutil\n",
        "import numpy as np\n",
        "import os, collections, sklearn\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "#Definition of some flags useful later in the code\n",
        "\n",
        "number_edges \u003d 8\n",
        "metric \u003d \u0027euclidean\u0027\n",
        "normalized_laplacian \u003d True\n",
        "coarsening_levels \u003d 4\n",
        "# Directories.\n",
        "dir_data \u003d \u0027data_mnist\u0027\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n",
        "#for each level\n",
        "\n",
        "def grid_graph(m):\n",
        "    z \u003d graph.grid(m)  # normalized nodes coordinates\n",
        "    dist, idx_ \u003d graph.distance_sklearn_metrics(z, k\u003dnumber_edges, metric\u003dmetric) \n",
        "    #dist contains the distance of the 8 nearest neighbors for each node indicated in z sorted in ascending order\n",
        "    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n",
        "\n",
        "    A_ \u003d graph.adjacency(dist, idx_)  # graph.adjacency() builds a sparse matrix out of the identified edges computing similarities as: A_{ij} \u003d e^(-dist_{ij}^2/sigma^2)\n",
        "    \n",
        "    return A_, z\n",
        "\n",
        "def coarsen(A_, levels): # Equivalent to the pooling layer\n",
        "    graphs, parents \u003d coarsening.metis(A_, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n",
        "                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n",
        "                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n",
        "                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n",
        "                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n",
        "                                                  #in the choice of the neighbor of each node.\n",
        "                    \n",
        "                                                  #Construction is done a priori, so we have one graph for all the samples!\n",
        "                    \n",
        "                                                  #graphs \u003d list of spare adjacency matrices (it contains in position \n",
        "                                                  #          0 the original graph)\n",
        "                                                  #parents \u003d list of numpy arrays (every array in position i contains \n",
        "                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n",
        "                                                  #           node i in the coarsed graph -\u003e that is, the idx of its cluster) \n",
        "    perms \u003d coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n",
        "                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n",
        "                                             #to produce the coarsed version of the graph.\n",
        "                                             #Fake nodes are appended for each node which is not grouped with anybody else\n",
        "    laplacians \u003d []\n",
        "    for i,A__ in enumerate(graphs):\n",
        "        M_, M_ \u003d A__.shape\n",
        "\n",
        "        # We remove self-connections created by metis.\n",
        "        A__ \u003d A__.tocoo()\n",
        "        A__.setdiag(0)\n",
        "\n",
        "        if i \u003c levels: #if we have to pool the graph \n",
        "            A__ \u003d coarsening.perm_adjacency(A__, perms[i]) #matrix A is here extended with the fakes nodes\n",
        "                                                       #in order to do an efficient pooling operation\n",
        "                                                       #in tensorflow as it was a 1D pooling\n",
        "\n",
        "        A__ \u003d A__.tocsr()\n",
        "        A__.eliminate_zeros()\n",
        "        Mnew, Mnew \u003d A__.shape\n",
        "        print(\u0027Layer {0}: M_{0} \u003d |V| \u003d {1} nodes ({2} added), |E| \u003d {3} edges\u0027.format(i, Mnew, Mnew-M_, A__.nnz//2))\n",
        "\n",
        "        L_ \u003d graph.laplacian(A__, normalized\u003dnormalized_laplacian)\n",
        "        laplacians.append(L_)\n",
        "\n",
        "    return laplacians, perms[0] if len(perms) \u003e 0 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def load_mnist():\n",
        "    #loading of MNIST dataset\n",
        "    np.random.seed(0)\n",
        "    n_rows_cols \u003d 28\n",
        "    A_, nodes_coordinates \u003d grid_graph(n_rows_cols)\n",
        "    L_, perm \u003d coarsen(A_, coarsening_levels)\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist_ \u003d input_data.read_data_sets(dir_data, one_hot\u003dFalse)\n",
        "    train_data_ \u003d mnist_.train.images.astype(np.float32)\n",
        "    val_data_ \u003d mnist_.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n",
        "    #are used for validation\n",
        "    test_data_ \u003d mnist_.test.images.astype(np.float32)\n",
        "    train_labels_ \u003d mnist_.train.labels\n",
        "    val_labels_ \u003d mnist_.validation.labels\n",
        "    test_labels_ \u003d mnist_.test.labels\n",
        "    # t_start \u003d time.time()\n",
        "    train_data_ \u003d coarsening.perm_data(train_data_, perm)\n",
        "    val_data_ \u003d coarsening.perm_data(val_data_, perm)\n",
        "    test_data_ \u003d coarsening.perm_data(test_data_, perm)\n",
        "    # print(\u0027Execution time: {:.2f}s\u0027.format(time.time() - t_start))\n",
        "    L_norm \u003d []\n",
        "    for k in range(len(L_)):\n",
        "        L_norm.append(L_[k] - sp.eye(L_[k].shape[0]))\n",
        "    del perm\n",
        "    return L_, mnist_, test_data_, test_labels_, train_data_, train_labels_, val_data_, val_labels_, L_norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-1-49a2f70a7839\u003e\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    def MyMoNetConv(self, x):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ],
          "evalue": "expected an indented block (\u003cipython-input-1-49a2f70a7839\u003e, line 50)",
          "ename": "IndentationError",
          "output_type": "error"
        }
      ],
      "source": "# class ChebNet:\nclass MyMoNet:\n    \"\"\"\n    The neural network model.\n    \"\"\"\n    \n    #Helper functions used for constructing the model\n    def _weight_variable(self, shape, regularization\u003dTrue): \n        \"\"\"Initializer for the weights\"\"\"\n        \n        initial \u003d tf.truncated_normal_initializer(0, 0.1)\n        var \u003d tf.get_variable(\u0027weights\u0027, shape, tf.float32, initializer\u003dinitial)\n        if regularization: #append the loss of the current variable to the regularization term \n            self.regularizers.append(tf.nn.l2_loss(var))\n        return var\n    \n    def _bias_variable(self, shape, regularization\u003dTrue):\n        \"\"\"Initializer for the bias\"\"\"\n        \n        initial \u003d tf.constant_initializer(0.1)\n        var \u003d tf.get_variable(\u0027bias\u0027, shape, tf.float32, initializer\u003dinitial)\n        if regularization:\n            self.regularizers.append(tf.nn.l2_loss(var))\n        return var\n    \n\n    def frobenius_norm(self, tensor): \n        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n        \n        square_tensor \u003d tf.square(tensor)\n        tensor_sum \u003d tf.reduce_sum(square_tensor)\n        frobenius_norm \u003d tf.sqrt(tensor_sum)\n        return frobenius_norm\n    \n    \n    def count_no_weights(self):\n        total_parameters \u003d 0\n        for variable in tf.trainable_variables():\n            # shape is an array of tf.Dimension\n            shape \u003d variable.get_shape()\n            variable_parameters \u003d 1\n            for dim in shape:\n                variable_parameters *\u003d dim.value\n            total_parameters +\u003d variable_parameters\n        print(\u0027#weights in the model: %d\u0027 % (total_parameters,))\n\n    def patch_operator(self,j, x): # D_j(x)f patch operator\n        with tf.variable_scope(\u0027guassian_filter{}\u0027.format(j)):  #aka the kernel\n            mu_j \u003d tf.get_variable(\u0027mu_{}\u0027.format(j), shape\u003d[1,self.dim_d])\n            sigma_j \u003d tf.get_variable(\u0027sigma_{}\u0027.format(j), shape\u003d[1,self.dim_d])\n            w_j\u003d  tf.exp((-1/2)*(self.u-mu_j).T*sigma_j^-1*(self.u-mu_j), name\u003d\u0027w_{}\u0027.format(j))\n        d_j\u003dtf.reduce_sum(w_j*x)\n        return d_j\n        \n    \n    def MyMoNetConv(self, x): \n        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n        N, M, Fin \u003d x.get_shape()  # N is the number of images # batch size \n                                   # M the number of vertices in the images\n                                   # Fin the number of features\n        d_j_list\u003d[]\n        for j in range(self.K): # Cycle through the Gaussian\n            d_j \u003d self.patch_operator(j,x, Fin)\n            d_j_list.append(d_j)\n        d_j_conv\u003dtf.stack(d_j_list)\n        G \u003d self._weight_variable([Fin*K, Fout])\n        out_conv \u003d tf.matmul(d_j_conv, G)  # N*M x Fout\n        return tf.reshape(out_conv, [N, M, Fout])  # N x M x Fout\n\n    def b1relu(self, x):\n        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n        N, M, F \u003d x.get_shape()\n        b \u003d self._bias_variable([1, 1, int(F)], regularization\u003dFalse)\n        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n\n\n    def mpool1(self, x, p):\n        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n        if p \u003e 1:\n            x \u003d tf.expand_dims(x, 3)  # shape \u003d N x M x F x 1\n            x \u003d tf.nn.max_pool(x, ksize\u003d[1,p,1,1], strides\u003d[1,p,1,1], padding\u003d\u0027SAME\u0027)\n            return tf.squeeze(x, [3])  # shape \u003d N x M/p x F\n        else:\n            return x\n\n    def fc(self, x, Mout, relu\u003dTrue):\n        \"\"\"Fully connected layer with Mout features.\"\"\"\n        N, Min \u003d x.get_shape()\n        W \u003d self._weight_variable([int(Min), Mout], regularization\u003dTrue)\n        b \u003d self._bias_variable([Mout], regularization\u003dTrue)\n        x \u003d tf.matmul(x, W) + b\n        return tf.nn.relu(x) if relu else x\n    \n    #function used for extracting the result of our model\n    def _inference(self, x, dropout): #definition of the model\n       # Graph convolutional layers.\n        x \u003d tf.expand_dims(x, 2)  # N x M x F\u003d1\n        for i in range(len(self.p)):\n            with tf.variable_scope(\u0027cgconv{}\u0027.format(i+1)):\n                with tf.name_scope(\u0027filter\u0027):\n                    conv_out \u003d self.MyMoNetConv(x)\n                    # x \u003d self.MyMoNetConv(x, self.idx_rows[i*2], self.idx_cols[i*2], self.edge_feat[i*2], self.list_A[i*2].shape, self.list_kernel_std[i*2], self.F[i], self.K[i])\n                with tf.name_scope(\u0027bias_relu\u0027):\n                    relu_out \u003d self.b1relu(conv_out)\n                with tf.name_scope(\u0027pooling\u0027):\n                    pool_out \u003d self.mpool1(relu_out, self.p[i])\n        \n        # Fully connected hidden layers.\n        N, M, F \u003d pool_out.get_shape()\n        pool_out_reshaped \u003d tf.reshape(pool_out, [int(N), int(M*F)])  # N x M\n        for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n                                           #(we discard the last value in M since it contains the number of classes we have\n                                           #to predict)\n            with tf.variable_scope(\u0027fc{}\u0027.format(i+1)):\n                fc_out \u003d self.fc(pool_out_reshaped, M)\n                dropout_out \u003d tf.nn.dropout(fc_out, dropout)\n        \n        # Logits linear layer, i.e. softmax without normalization.\n        with tf.variable_scope(\u0027logits\u0027):\n            logits \u003d self.fc(dropout_out, self.M[-1], relu\u003dFalse)\n        return logits\n    \n    \n    def convert_coo_to_sparse_tensor(self, L):\n        indices \u003d np.column_stack((L.row, L.col))\n        L \u003d tf.SparseTensor(indices, L.data.astype(\u0027float32\u0027), L.shape)\n        L \u003d tf.sparse_reorder(L)\n        return L\n        \n    \n    def __init__(self, p, K, F, M, M_0, batch_size, L,u ,decay_steps, decay_rate, learning_rate\u003d1e-4, momentum\u003d0.9, regularization\u003d5e-4, idx_gpu \u003d \u0027/gpu:0\u0027):\n        self.regularizers \u003d list()  # list of regularization l2 loss for multiple variables\n        self.p \u003d p                  # dimensions of the pooling layers\n        self.K \u003d K                  # Number of Gaussian used \n        self.dim_d \u003d 2                # dimensionality of function u(x,y) with x vertex and y vertex in N(x)\n        self.F \u003d F                  # Number of features of convolutional layers\n        self.M \u003d M                  # Number of neurons in fully connected layers\n        self.M_0 \u003d M_0              # number of elements in the first graph \n        self.batch_size \u003d batch_size\n        \n                        #definition of some learning parameters\n        self.decay_steps \u003d decay_steps\n        self.decay_rate \u003d decay_rate\n        self.learning_rate \u003d learning_rate\n        self.regularization \u003d regularization\n        \n        ## checked______________________________\n        \n        with tf.Graph().as_default() as g:\n                self.graph \u003d g\n                tf.set_random_seed(0)\n                with tf.device(idx_gpu):\n                        self.u \u003d u \n                        #definition of placeholders\n                        self.L \u003d [self.convert_coo_to_sparse_tensor(c_L.tocoo()) for c_L in L]\n                        self.ph_data \u003d tf.placeholder(tf.float32, (self.batch_size, M_0), \u0027data\u0027)\n                        self.ph_labels \u003d tf.placeholder(tf.int32, (self.batch_size), \u0027labels\u0027)\n                        self.ph_dropout \u003d tf.placeholder(tf.float32, (), \u0027dropout\u0027)\n                    \n                        #Model construction\n                        self.logits \u003d self._inference(self.ph_data, self.ph_dropout)\n                        \n                        #Definition of the loss function\n                        with tf.name_scope(\u0027loss\u0027):\n                            self.cross_entropy \u003d tf.nn.sparse_softmax_cross_entropy_with_logits(logits\u003dself.logits, \n                                                                                                labels\u003dself.ph_labels)\n                            self.cross_entropy \u003d tf.reduce_mean(self.cross_entropy)\n                        with tf.name_scope(\u0027regularization\u0027):\n                            self.regularization *\u003d tf.add_n(self.regularizers)\n                        self.loss \u003d self.cross_entropy + self.regularization\n                        \n                        #Solver Definition\n                        with tf.name_scope(\u0027training\u0027):\n                            # Learning rate.\n                            global_step \u003d tf.Variable(0, name\u003d\u0027global_step\u0027, trainable\u003dFalse) #used for counting how many iterations we have done\n                            if decay_rate !\u003d 1: #applies an exponential decay of the lr wrt the number of iterations done\n                                learning_rate \u003d tf.train.exponential_decay(\n                                        learning_rate, global_step, decay_steps, decay_rate, staircase\u003dTrue)\n                            # Optimizer.\n                            if momentum \u003d\u003d 0:\n                                optimizer \u003d tf.train.GradientDescentOptimizer(learning_rate)\n                            else: #applies momentum for increasing the robustness of the gradient \n                                optimizer \u003d tf.train.MomentumOptimizer(learning_rate, momentum)\n                            grads \u003d optimizer.compute_gradients(self.loss)\n                            self.op_gradients \u003d optimizer.apply_gradients(grads, global_step\u003dglobal_step) \n                            \n                        #Computation of the norm gradients (useful for debugging)\n                        self.var_grad \u003d tf.gradients(self.loss, tf.trainable_variables())\n                        self.norm_grad \u003d self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n\n                        #Extraction of the predictions and computation of accuracy\n                        self.predictions \u003d tf.cast(tf.argmax(self.logits, dimension\u003d1), tf.int32)\n                        self.accuracy \u003d 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n        \n                        # Create a session for running Ops on the Graph.\n                        config \u003d tf.ConfigProto(allow_soft_placement \u003d True)\n                        config.gpu_options.allow_growth \u003d True\n                        self.session \u003d tf.Session(config\u003dconfig)\n\n                        # Run the Op to initialize the variables.\n                        init \u003d tf.global_variables_initializer()\n                        self.session.run(init)\n                        \n                        self.count_no_weights()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "    L, mnist, test_data, test_labels, train_data, train_labels, val_data, val_labels, L_norm \u003d load_mnist()\n",
        "    #Convolutional parameters\n",
        "    p \u003d [4, 4]   #Dimensions of the pooling layers\n",
        "    K \u003d [25, 25] #List of polynomial orders, i.e. filter sizes or number of hops\n",
        "    F \u003d [32, 64] #Number of features of convolutional layers\n",
        "    \n",
        "    #FC parameters\n",
        "    C \u003d max(mnist.train.labels) + 1 #Number of classes we have\n",
        "    M \u003d [512, C] #Number of neurons in fully connected layers\n",
        "    \n",
        "    #Solver parameters\n",
        "    batch_size \u003d 100\n",
        "    decay_steps \u003d mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n",
        "    decay_rate \u003d 0.95 #how much decreasing the learning rate\n",
        "    learning_rate \u003d 0.02\n",
        "    momentum \u003d 0.9\n",
        "    regularization \u003d 5e-4\n",
        "    \n",
        "    #Definition of keep probabilities for dropout layers\n",
        "    dropout_training \u003d 0.5\n",
        "    dropout_val_test \u003d 1.0\n",
        "    #%%\n",
        "    #Construction of the learning obj\n",
        "    M_0 \u003d L[0].shape[0] #number of elements in the first graph\n",
        "    learning_obj \u003d MyMoNet(p, K, F, M, M_0, batch_size, L_norm,\n",
        "                           decay_steps, decay_rate,\n",
        "                           learning_rate\u003dlearning_rate, regularization\u003dregularization,\n",
        "                           momentum\u003dmomentum)\n",
        "    \n",
        "    #definition of overall number of training iterations and validation frequency\n",
        "    num_iter_val \u003d 600\n",
        "    num_total_iter_training \u003d 21000\n",
        "    \n",
        "    num_iter \u003d 0\n",
        "    \n",
        "    list_training_loss \u003d list()\n",
        "    list_training_norm_grad \u003d list()\n",
        "    list_val_accuracy \u003d list()\n",
        "    #%%\n",
        "    #training and validation\n",
        "    indices \u003d collections.deque() #queue that will contain a permutation of the training indexes\n",
        "    for k in range(num_iter, num_total_iter_training):\n",
        "        \n",
        "        #Construction of the training batch\n",
        "        if len(indices) \u003c batch_size: # Be sure to have used all the samples before using one a second time.\n",
        "            indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n",
        "        idx \u003d [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n",
        "        \n",
        "        #data extraction\n",
        "        batch_data, batch_labels \u003d train_data[idx,:], train_labels[idx] \n",
        "        \n",
        "        feed_dict \u003d {learning_obj.ph_data: batch_data, \n",
        "                     learning_obj.ph_labels: batch_labels, \n",
        "                     learning_obj.ph_dropout: dropout_training}\n",
        "        \n",
        "        #Training\n",
        "        tic \u003d time.time()\n",
        "        _, current_training_loss, norm_grad \u003d learning_obj.session.run([learning_obj.op_gradients, \n",
        "                                                                        learning_obj.loss, \n",
        "                                                                        learning_obj.norm_grad], feed_dict \u003d feed_dict) \n",
        "        training_time \u003d time.time() - tic\n",
        "        \n",
        "        list_training_loss.append(current_training_loss)\n",
        "        list_training_norm_grad.append(norm_grad)\n",
        "        \n",
        "        if (np.mod(num_iter, num_iter_val)\u003d\u003d0): #validation\n",
        "            msg \u003d \"[TRN] iter \u003d %03i, cost \u003d %3.2e, |grad| \u003d %.2e (%3.2es)\" \\\n",
        "                        % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n",
        "            print(msg)\n",
        "            \n",
        "            #Validation Code\n",
        "            tic \u003d time.time()\n",
        "            val_accuracy \u003d 0\n",
        "            for begin in range(0, val_data.shape[0], batch_size):\n",
        "                end \u003d begin + batch_size\n",
        "                end \u003d min([end, val_data.shape[0]])\n",
        "                \n",
        "                #data extraction\n",
        "                batch_data \u003d np.zeros((end-begin, val_data.shape[1]))\n",
        "                batch_data \u003d val_data[begin:end,:]\n",
        "                batch_labels \u003d np.zeros(batch_size)\n",
        "                batch_labels[:end-begin] \u003d val_labels[begin:end]\n",
        "                \n",
        "                feed_dict \u003d {learning_obj.ph_data: batch_data, \n",
        "                             learning_obj.ph_labels: batch_labels,\n",
        "                             learning_obj.ph_dropout: dropout_val_test}\n",
        "                \n",
        "                batch_accuracy \u003d learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
        "                val_accuracy +\u003d batch_accuracy*batch_data.shape[0]\n",
        "            val_accuracy \u003d val_accuracy/val_data.shape[0]\n",
        "            val_time \u003d time.time() - tic\n",
        "            msg \u003d \"[VAL] iter \u003d %03i, acc \u003d %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n",
        "            print(msg)\n",
        "        num_iter +\u003d 1\n",
        "    #Test code\n",
        "    tic \u003d time.time()\n",
        "    test_accuracy \u003d 0\n",
        "    for begin in range(0, test_data.shape[0], batch_size):\n",
        "        end \u003d begin + batch_size\n",
        "        end \u003d min([end, test_data.shape[0]])\n",
        "                \n",
        "        batch_data \u003d np.zeros((end-begin, test_data.shape[1]))\n",
        "        batch_data \u003d test_data[begin:end,:]\n",
        "                \n",
        "        feed_dict \u003d {learning_obj.ph_data: batch_data, learning_obj.ph_dropout: 1}\n",
        "                \n",
        "        batch_labels \u003d np.zeros(batch_size)\n",
        "        batch_labels[:end-begin] \u003d test_labels[begin:end]\n",
        "        feed_dict[learning_obj.ph_labels] \u003d batch_labels\n",
        "                \n",
        "        batch_accuracy \u003d learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
        "        test_accuracy +\u003d batch_accuracy*batch_data.shape[0]\n",
        "    test_accuracy \u003d test_accuracy/test_data.shape[0]\n",
        "    test_time \u003d time.time() - tic\n",
        "    msg \u003d \"[TST] iter \u003d %03i, acc \u003d %4.2f (%3.2es)\" % (num_iter, test_accuracy, test_time)\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: M_0 \u003d |V| \u003d 944 nodes (160 added), |E| \u003d 3198 edges\n",
            "Layer 1: M_1 \u003d |V| \u003d 472 nodes (64 added), |E| \u003d 1426 edges\n",
            "Layer 2: M_2 \u003d |V| \u003d 236 nodes (22 added), |E| \u003d 653 edges\n",
            "Layer 3: M_3 \u003d |V| \u003d 118 nodes (6 added), |E| \u003d 318 edges\n",
            "Layer 4: M_4 \u003d |V| \u003d 59 nodes (0 added), |E| \u003d 152 edges\n",
            "WARNING:tensorflow:From \u003cipython-input-4-6a009cd2e1fd\u003e:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data_mnist/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data_mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting data_mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From \u003cipython-input-5-09dd0f8ec1a9\u003e:259: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "#weights in the model: 1991050\n",
            "[TRN] iter \u003d 000, cost \u003d 1.14e+01, |grad| \u003d 2.72e+01 (1.89e+00s)\n",
            "[VAL] iter \u003d 000, acc \u003d 11.36 (1.49e+00s)\n",
            "[TRN] iter \u003d 600, cost \u003d 3.63e+00, |grad| \u003d 1.34e+00 (5.78e-02s)\n",
            "[VAL] iter \u003d 600, acc \u003d 96.92 (1.30e+00s)\n",
            "[TRN] iter \u003d 1200, cost \u003d 3.11e+00, |grad| \u003d 1.03e+00 (5.44e-02s)\n",
            "[VAL] iter \u003d 1200, acc \u003d 97.70 (1.28e+00s)\n",
            "[TRN] iter \u003d 1800, cost \u003d 2.82e+00, |grad| \u003d 8.66e-01 (5.48e-02s)\n",
            "[VAL] iter \u003d 1800, acc \u003d 97.98 (1.29e+00s)\n",
            "[TRN] iter \u003d 2400, cost \u003d 2.56e+00, |grad| \u003d 1.23e+00 (5.53e-02s)\n",
            "[VAL] iter \u003d 2400, acc \u003d 98.42 (1.28e+00s)\n",
            "[TRN] iter \u003d 3000, cost \u003d 2.31e+00, |grad| \u003d 8.89e-01 (5.49e-02s)\n",
            "[VAL] iter \u003d 3000, acc \u003d 98.42 (1.30e+00s)\n",
            "[TRN] iter \u003d 3600, cost \u003d 2.13e+00, |grad| \u003d 1.09e+00 (5.51e-02s)\n",
            "[VAL] iter \u003d 3600, acc \u003d 98.66 (1.28e+00s)\n",
            "[TRN] iter \u003d 4200, cost \u003d 1.89e+00, |grad| \u003d 7.06e-01 (5.46e-02s)\n",
            "[VAL] iter \u003d 4200, acc \u003d 98.62 (1.29e+00s)\n",
            "[TRN] iter \u003d 4800, cost \u003d 1.75e+00, |grad| \u003d 6.32e-01 (5.54e-02s)\n",
            "[VAL] iter \u003d 4800, acc \u003d 98.74 (1.30e+00s)\n",
            "[TRN] iter \u003d 5400, cost \u003d 1.63e+00, |grad| \u003d 4.86e-01 (5.46e-02s)\n",
            "[VAL] iter \u003d 5400, acc \u003d 98.78 (1.28e+00s)\n",
            "[TRN] iter \u003d 6000, cost \u003d 1.51e+00, |grad| \u003d 6.11e-01 (5.60e-02s)\n",
            "[VAL] iter \u003d 6000, acc \u003d 98.76 (1.29e+00s)\n",
            "[TRN] iter \u003d 6600, cost \u003d 1.40e+00, |grad| \u003d 4.16e-01 (5.61e-02s)\n",
            "[VAL] iter \u003d 6600, acc \u003d 98.92 (1.30e+00s)\n",
            "[TRN] iter \u003d 7200, cost \u003d 1.36e+00, |grad| \u003d 1.07e+00 (5.55e-02s)\n",
            "[VAL] iter \u003d 7200, acc \u003d 98.86 (1.34e+00s)\n",
            "[TRN] iter \u003d 7800, cost \u003d 1.23e+00, |grad| \u003d 3.22e-01 (5.56e-02s)\n",
            "[VAL] iter \u003d 7800, acc \u003d 98.92 (1.29e+00s)\n",
            "[TRN] iter \u003d 8400, cost \u003d 1.18e+00, |grad| \u003d 6.71e-01 (5.46e-02s)\n",
            "[VAL] iter \u003d 8400, acc \u003d 99.02 (1.29e+00s)\n",
            "[TRN] iter \u003d 9000, cost \u003d 1.15e+00, |grad| \u003d 7.70e-01 (5.84e-02s)\n",
            "[VAL] iter \u003d 9000, acc \u003d 98.88 (1.29e+00s)\n",
            "[TRN] iter \u003d 9600, cost \u003d 1.08e+00, |grad| \u003d 1.20e+00 (5.46e-02s)\n",
            "[VAL] iter \u003d 9600, acc \u003d 99.00 (1.30e+00s)\n",
            "[TRN] iter \u003d 10200, cost \u003d 1.00e+00, |grad| \u003d 3.76e-01 (5.74e-02s)\n",
            "[VAL] iter \u003d 10200, acc \u003d 98.86 (1.32e+00s)\n",
            "[TRN] iter \u003d 10800, cost \u003d 9.83e-01, |grad| \u003d 9.74e-01 (5.48e-02s)\n",
            "[VAL] iter \u003d 10800, acc \u003d 98.92 (1.31e+00s)\n",
            "[TRN] iter \u003d 11400, cost \u003d 9.24e-01, |grad| \u003d 4.74e-01 (5.46e-02s)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-7-ec9775ede022\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m\u003cipython-input-6-e8bb8e70c3b1\u003e\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m                              learning_obj.ph_dropout: dropout_val_test}\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 88\u001b[0;31m                 \u001b[0mbatch_accuracy\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mlearning_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mval_accuracy\u001b[0m \u001b[0;34m+\u003d\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mval_accuracy\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result \u003d self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--\u003e 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results \u003d self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-\u003e 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-\u003e 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-\u003e 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (GDL_HW1)",
      "language": "python",
      "name": "pycharm-56591012"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}