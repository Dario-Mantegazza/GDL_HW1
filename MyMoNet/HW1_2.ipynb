{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#import graph, coarsening, utils\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport graph\n",
    "%aimport coarsening\n",
    "%aimport utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import time, shutil\n",
    "import numpy as np\n",
    "import os, collections, sklearn\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Definition of some flags useful later in the code\n",
    "\n",
    "number_edges = 8\n",
    "metric = 'euclidean'\n",
    "normalized_laplacian = True\n",
    "coarsening_levels = 4\n",
    "# Directories.\n",
    "dir_data = 'data_mnist'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n",
    "#for each level\n",
    "\n",
    "def grid_graph(m):\n",
    "    z = graph.grid(m)  # normalized nodes coordinates\n",
    "    dist, idx_ = graph.distance_sklearn_metrics(z, k=number_edges, metric=metric) \n",
    "    #dist contains the distance of the 8 nearest neighbors for each node indicated in z sorted in ascending order\n",
    "    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n",
    "\n",
    "    A_ = graph.adjacency(dist, idx_)  # graph.adjacency() builds a sparse matrix out of the identified edges computing similarities as: A_{ij} = e^(-dist_{ij}^2/sigma^2)\n",
    "    \n",
    "    return A_, z\n",
    "\n",
    "def coarsen(A_, levels): # Equivalent to the pooling layer\n",
    "    graphs, parents = coarsening.metis(A_, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n",
    "                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n",
    "                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n",
    "                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n",
    "                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n",
    "                                                  #in the choice of the neighbor of each node.\n",
    "                    \n",
    "                                                  #Construction is done a priori, so we have one graph for all the samples!\n",
    "                    \n",
    "                                                  #graphs = list of spare adjacency matrices (it contains in position \n",
    "                                                  #          0 the original graph)\n",
    "                                                  #parents = list of numpy arrays (every array in position i contains \n",
    "                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n",
    "                                                  #           node i in the coarsed graph -> that is, the idx of its cluster) \n",
    "    perms = coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n",
    "                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n",
    "                                             #to produce the coarsed version of the graph.\n",
    "                                             #Fake nodes are appended for each node which is not grouped with anybody else\n",
    "    laplacians = []\n",
    "    for i,A__ in enumerate(graphs):\n",
    "        M_, M_ = A__.shape\n",
    "\n",
    "        # We remove self-connections created by metis.\n",
    "        A__ = A__.tocoo()\n",
    "        A__.setdiag(0)\n",
    "\n",
    "        if i < levels: #if we have to pool the graph \n",
    "            A__ = coarsening.perm_adjacency(A__, perms[i]) #matrix A is here extended with the fakes nodes\n",
    "                                                       #in order to do an efficient pooling operation\n",
    "                                                       #in tensorflow as it was a 1D pooling\n",
    "\n",
    "        A__ = A__.tocsr()\n",
    "        A__.eliminate_zeros()\n",
    "        Mnew, Mnew = A__.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M_, A__.nnz//2))\n",
    "\n",
    "        L_ = graph.laplacian(A__, normalized=normalized_laplacian)\n",
    "        laplacians.append(L_)\n",
    "\n",
    "    return laplacians, perms[0] if len(perms) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    #loading of MNIST dataset\n",
    "    np.random.seed(0)\n",
    "    n_rows_cols = 28\n",
    "    A_, nodes_coordinates = grid_graph(n_rows_cols)\n",
    "    L_, perm = coarsen(A_, coarsening_levels)\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist_ = input_data.read_data_sets(dir_data, one_hot=False)\n",
    "    train_data_ = mnist_.train.images.astype(np.float32)\n",
    "    val_data_ = mnist_.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n",
    "    #are used for validation\n",
    "    test_data_ = mnist_.test.images.astype(np.float32)\n",
    "    train_labels_ = mnist_.train.labels\n",
    "    val_labels_ = mnist_.validation.labels\n",
    "    test_labels_ = mnist_.test.labels\n",
    "    # t_start = time.time()\n",
    "    train_data_ = coarsening.perm_data(train_data_, perm)\n",
    "    val_data_ = coarsening.perm_data(val_data_, perm)\n",
    "    test_data_ = coarsening.perm_data(test_data_, perm)\n",
    "    # print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "    L_norm = []\n",
    "    for k in range(len(L_)):\n",
    "        L_norm.append(L_[k] - sp.eye(L_[k].shape[0]))\n",
    "    del perm\n",
    "    return L_, mnist_, test_data_, test_labels_, train_data_, train_labels_, val_data_, val_labels_, L_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# class ChebNet:\n",
    "class MyMoNet:\n",
    "    \"\"\"\n",
    "    The neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Helper functions used for constructing the model\n",
    "    def _weight_variable(self, shape, regularization=True): \n",
    "        \"\"\"Initializer for the weights\"\"\"\n",
    "        \n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n",
    "        if regularization: #append the loss of the current variable to the regularization term \n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        \"\"\"Initializer for the bias\"\"\"\n",
    "        \n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "\n",
    "    def frobenius_norm(self, tensor): \n",
    "        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n",
    "        \n",
    "        square_tensor = tf.square(tensor)\n",
    "        tensor_sum = tf.reduce_sum(square_tensor)\n",
    "        frobenius_norm = tf.sqrt(tensor_sum)\n",
    "        return frobenius_norm\n",
    "    \n",
    "    \n",
    "    def count_no_weights(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        print('#weights in the model: %d' % (total_parameters,))\n",
    "\n",
    "    \n",
    "    #Modules used by the graph convolutional network\n",
    "    # def chebyshevConv(self, x, L, Fout, K): \n",
    "    #     \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n",
    "    #     \n",
    "    #     N, M, Fin = x.get_shape()  # N is the number of images\n",
    "    #                                # M the number of vertices in the images\n",
    "    #                                # Fin the number of features\n",
    "    #     N, M, Fin = int(N), int(M), int(Fin)\n",
    "    #     \n",
    "    #     # Transform to Chebyshev basis. Here we apply the chebyshev polynomials. For simplicity instead of operating\n",
    "    #     # over the eigenvalues of the laplacians we apply directly the chebysev polynomials to the laplacians.\n",
    "    #     # This makes everything more efficient if the laplacians are actually represented as sparse matrices.\n",
    "    #     x0 = x # L^0 * x = I * x\n",
    "    #     x = tf.expand_dims(x0, 0)  # shape = 1 x N x M x Fin\n",
    "    #     def concat(x, x_):\n",
    "    #         x_ = tf.expand_dims(x_, 0)  # shape = 1 x M x Fin*N\n",
    "    #         return tf.concat([x, x_], 0)  # shape = 1 x N x M x Fin\n",
    "    #     if K > 1:\n",
    "    #         x0 = tf.transpose(x0, [1,2,0])\n",
    "    #         x0 = tf.reshape(x0, [M, Fin*N])\n",
    "    #         x1 = tf.sparse_tensor_dense_matmul(L, x0) #shape = M x Fin*N <- L^1 * x\n",
    "    #         x1 = tf.reshape(x1, [M, Fin, N]) # shape = M x Fin x N <- L^1 * x\n",
    "    #         x1 = tf.transpose(x1, [2,0,1]) # shape = N x M x Fin\n",
    "    #         x0 = tf.reshape(x0, [M, Fin, N])\n",
    "    #         x0 = tf.transpose(x0, [2,0,1]) # shape = N x M x Fin\n",
    "    #         x = concat(x, x1) # shape = 2 x N x M x Fin\n",
    "    #     for k in range(2, K):\n",
    "    #         x1 = tf.transpose(x1, [1,2,0])\n",
    "    #         x1 = tf.reshape(x1, [M, Fin*N])\n",
    "    #         x2 = tf.sparse_tensor_dense_matmul(L, x1) # shape = M x Fin*N <- L^k * x\n",
    "    #         x2 = tf.reshape(x2, [M, Fin, N]) # shape = M x Fin x N <- L^1 * x\n",
    "    #         x2 = tf.transpose(x2, [2,0,1])\n",
    "    #         x1 = tf.reshape(x1, [M, Fin, N])\n",
    "    #         x1 = tf.transpose(x1, [2,0,1]) # shape = N x M x Fin\n",
    "    #         x2 = 2 * x2 - x0  # shape = N x M x Fin <- recursive definition of chebyshev polynomials\n",
    "    #         x = concat(x, x2) # shape = K x N x M x Fin\n",
    "    #         x0, x1 = x1, x2\n",
    "    #     x = tf.transpose(x, [1,2,3,0]) # shape = N x M x Fin x K\n",
    "    #     x = tf.reshape(x, [N*M, Fin*K])  # shape = N*M x Fin*K\n",
    "    #     \n",
    "    #     # Filter: Fout filters of order K applied over all the Fin features\n",
    "    #     W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "    #     x = tf.matmul(x, W)  # N*M x Fout\n",
    "    #     return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "    def chebyshevConv(self, x, L, Fout, K): \n",
    "        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n",
    "        \n",
    "        N, M, Fin = x.get_shape()  # N is the number of images\n",
    "                                   # M the number of vertices in the images\n",
    "                                   # Fin the number of features\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        \n",
    "        # Transform to Chebyshev basis. Here we apply the chebyshev polynomials. For simplicity instead of operating\n",
    "        # over the eigenvalues of the laplacians we apply directly the chebysev polynomials to the laplacians.\n",
    "        # This makes everything more efficient if the laplacians are actually represented as sparse matrices.\n",
    "        x0 = x # L^0 * x = I * x\n",
    "        x = tf.expand_dims(x0, 0)  # shape = 1 x N x M x Fin\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)  # shape = 1 x M x Fin*N\n",
    "            return tf.concat([x, x_], 0)  # shape = 1 x N x M x Fin\n",
    "        if K > 1:\n",
    "            x0 = tf.transpose(x0, [1,2,0])\n",
    "            x0 = tf.reshape(x0, [M, Fin*N])\n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0) #shape = M x Fin*N <- L^1 * x\n",
    "            x1 = tf.reshape(x1, [M, Fin, N]) # shape = M x Fin x N <- L^1 * x\n",
    "            x1 = tf.transpose(x1, [2,0,1]) # shape = N x M x Fin\n",
    "            x0 = tf.reshape(x0, [M, Fin, N])\n",
    "            x0 = tf.transpose(x0, [2,0,1]) # shape = N x M x Fin\n",
    "            x = concat(x, x1) # shape = 2 x N x M x Fin\n",
    "        for k in range(2, K):\n",
    "            x1 = tf.transpose(x1, [1,2,0])\n",
    "            x1 = tf.reshape(x1, [M, Fin*N])\n",
    "            x2 = tf.sparse_tensor_dense_matmul(L, x1) # shape = M x Fin*N <- L^k * x\n",
    "            x2 = tf.reshape(x2, [M, Fin, N]) # shape = M x Fin x N <- L^1 * x\n",
    "            x2 = tf.transpose(x2, [2,0,1])\n",
    "            x1 = tf.reshape(x1, [M, Fin, N])\n",
    "            x1 = tf.transpose(x1, [2,0,1]) # shape = N x M x Fin\n",
    "            x2 = 2 * x2 - x0  # shape = N x M x Fin <- recursive definition of chebyshev polynomials\n",
    "            x = concat(x, x2) # shape = K x N x M x Fin\n",
    "            x0, x1 = x1, x2\n",
    "        x = tf.transpose(x, [1,2,3,0]) # shape = N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # shape = N*M x Fin*K\n",
    "        \n",
    "        # Filter: Fout filters of order K applied over all the Fin features\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n",
    "\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # shape = N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # shape = N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "    \n",
    "    #function used for extracting the result of our model\n",
    "    def _inference(self, x, dropout): #definition of the model\n",
    "        \n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.variable_scope('cgconv{}'.format(i+1)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    x = self.chebyshevConv(x, self.L[i*2], self.F[i], self.K[i])\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.b1relu(x)\n",
    "                with tf.name_scope('pooling'):\n",
    "                    x = self.mpool1(x, self.p[i])\n",
    "         \n",
    "        # Fully connected hidden layers.\n",
    "        N, M, F = x.get_shape()\n",
    "        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n",
    "        for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n",
    "                                           #(we discard the last value in M since it contains the number of classes we have\n",
    "                                           #to predict)\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                x = self.fc(x, M)\n",
    "                x = tf.nn.dropout(x, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = self.fc(x, self.M[-1], relu=False)\n",
    "        return x\n",
    "    \n",
    "    def convert_coo_to_sparse_tensor(self, L):\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n",
    "        L = tf.sparse_reorder(L)\n",
    "        return L\n",
    "        \n",
    "    \n",
    "    def __init__(self, p, K, F, M, M_0, batch_size, L,\n",
    "                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, \n",
    "                 idx_gpu = '/gpu:0'):\n",
    "        self.regularizers = list() #list of regularization l2 loss for multiple variables\n",
    "        \n",
    "        self.p = p #dimensions of the pooling layers\n",
    "        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n",
    "        self.F = F #Number of features of convolutional layers\n",
    "        \n",
    "        self.M = M #Number of neurons in fully connected layers\n",
    "        \n",
    "        self.M_0 = M_0 #number of elements in the first graph \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #definition of some learning parameters\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        with tf.Graph().as_default() as g:\n",
    "                self.graph = g\n",
    "                tf.set_random_seed(0)\n",
    "                with tf.device(idx_gpu):\n",
    "                        #definition of placeholders\n",
    "                        self.L = [self.convert_coo_to_sparse_tensor(c_L.tocoo()) for c_L in L]\n",
    "                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
    "                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "                    \n",
    "                        #Model construction\n",
    "                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n",
    "                        \n",
    "                        #Definition of the loss function\n",
    "                        with tf.name_scope('loss'):\n",
    "                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                                                labels=self.ph_labels)\n",
    "                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n",
    "                        with tf.name_scope('regularization'):\n",
    "                            self.regularization *= tf.add_n(self.regularizers)\n",
    "                        self.loss = self.cross_entropy + self.regularization\n",
    "                        \n",
    "                        #Solver Definition\n",
    "                        with tf.name_scope('training'):\n",
    "                            # Learning rate.\n",
    "                            global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n",
    "                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n",
    "                                learning_rate = tf.train.exponential_decay(\n",
    "                                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "                            # Optimizer.\n",
    "                            if momentum == 0:\n",
    "                                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                            else: #applies momentum for increasing the robustness of the gradient \n",
    "                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "                            grads = optimizer.compute_gradients(self.loss)\n",
    "                            self.op_gradients = optimizer.apply_gradients(grads, global_step=global_step) \n",
    "                            \n",
    "                        #Computation of the norm gradients (useful for debugging)\n",
    "                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n",
    "                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n",
    "\n",
    "                        #Extraction of the predictions and computation of accuracy\n",
    "                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n",
    "                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n",
    "        \n",
    "                        # Create a session for running Ops on the Graph.\n",
    "                        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "                        config.gpu_options.allow_growth = True\n",
    "                        self.session = tf.Session(config=config)\n",
    "\n",
    "                        # Run the Op to initialize the variables.\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        self.session.run(init)\n",
    "                        \n",
    "                        self.count_no_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    L, mnist, test_data, test_labels, train_data, train_labels, val_data, val_labels, L_norm = load_mnist()\n",
    "    #Convolutional parameters\n",
    "    p = [4, 4]   #Dimensions of the pooling layers\n",
    "    K = [25, 25] #List of polynomial orders, i.e. filter sizes or number of hops\n",
    "    F = [32, 64] #Number of features of convolutional layers\n",
    "    \n",
    "    #FC parameters\n",
    "    C = max(mnist.train.labels) + 1 #Number of classes we have\n",
    "    M = [512, C] #Number of neurons in fully connected layers\n",
    "    \n",
    "    #Solver parameters\n",
    "    batch_size = 100\n",
    "    decay_steps = mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n",
    "    decay_rate = 0.95 #how much decreasing the learning rate\n",
    "    learning_rate = 0.02\n",
    "    momentum = 0.9\n",
    "    regularization = 5e-4\n",
    "    \n",
    "    #Definition of keep probabilities for dropout layers\n",
    "    dropout_training = 0.5\n",
    "    dropout_val_test = 1.0\n",
    "    #%%\n",
    "    #Construction of the learning obj\n",
    "    M_0 = L[0].shape[0] #number of elements in the first graph\n",
    "    learning_obj = MyMoNet(p, K, F, M, M_0, batch_size, L_norm,\n",
    "                           decay_steps, decay_rate,\n",
    "                           learning_rate=learning_rate, regularization=regularization,\n",
    "                           momentum=momentum)\n",
    "    \n",
    "    #definition of overall number of training iterations and validation frequency\n",
    "    num_iter_val = 600\n",
    "    num_total_iter_training = 21000\n",
    "    \n",
    "    num_iter = 0\n",
    "    \n",
    "    list_training_loss = list()\n",
    "    list_training_norm_grad = list()\n",
    "    list_val_accuracy = list()\n",
    "    #%%\n",
    "    #training and validation\n",
    "    indices = collections.deque() #queue that will contain a permutation of the training indexes\n",
    "    for k in range(num_iter, num_total_iter_training):\n",
    "        \n",
    "        #Construction of the training batch\n",
    "        if len(indices) < batch_size: # Be sure to have used all the samples before using one a second time.\n",
    "            indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n",
    "        idx = [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n",
    "        \n",
    "        #data extraction\n",
    "        batch_data, batch_labels = train_data[idx,:], train_labels[idx] \n",
    "        \n",
    "        feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                     learning_obj.ph_labels: batch_labels, \n",
    "                     learning_obj.ph_dropout: dropout_training}\n",
    "        \n",
    "        #Training\n",
    "        tic = time.time()\n",
    "        _, current_training_loss, norm_grad = learning_obj.session.run([learning_obj.op_gradients, \n",
    "                                                                        learning_obj.loss, \n",
    "                                                                        learning_obj.norm_grad], feed_dict = feed_dict) \n",
    "        training_time = time.time() - tic\n",
    "        \n",
    "        list_training_loss.append(current_training_loss)\n",
    "        list_training_norm_grad.append(norm_grad)\n",
    "        \n",
    "        if (np.mod(num_iter, num_iter_val)==0): #validation\n",
    "            msg = \"[TRN] iter = %03i, cost = %3.2e, |grad| = %.2e (%3.2es)\" \\\n",
    "                        % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n",
    "            print(msg)\n",
    "            \n",
    "            #Validation Code\n",
    "            tic = time.time()\n",
    "            val_accuracy = 0\n",
    "            for begin in range(0, val_data.shape[0], batch_size):\n",
    "                end = begin + batch_size\n",
    "                end = min([end, val_data.shape[0]])\n",
    "                \n",
    "                #data extraction\n",
    "                batch_data = np.zeros((end-begin, val_data.shape[1]))\n",
    "                batch_data = val_data[begin:end,:]\n",
    "                batch_labels = np.zeros(batch_size)\n",
    "                batch_labels[:end-begin] = val_labels[begin:end]\n",
    "                \n",
    "                feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                             learning_obj.ph_labels: batch_labels,\n",
    "                             learning_obj.ph_dropout: dropout_val_test}\n",
    "                \n",
    "                batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "                val_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "            val_accuracy = val_accuracy/val_data.shape[0]\n",
    "            val_time = time.time() - tic\n",
    "            msg = \"[VAL] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n",
    "            print(msg)\n",
    "        num_iter += 1\n",
    "    #Test code\n",
    "    tic = time.time()\n",
    "    test_accuracy = 0\n",
    "    for begin in range(0, test_data.shape[0], batch_size):\n",
    "        end = begin + batch_size\n",
    "        end = min([end, test_data.shape[0]])\n",
    "                \n",
    "        batch_data = np.zeros((end-begin, test_data.shape[1]))\n",
    "        batch_data = test_data[begin:end,:]\n",
    "                \n",
    "        feed_dict = {learning_obj.ph_data: batch_data, learning_obj.ph_dropout: 1}\n",
    "                \n",
    "        batch_labels = np.zeros(batch_size)\n",
    "        batch_labels[:end-begin] = test_labels[begin:end]\n",
    "        feed_dict[learning_obj.ph_labels] = batch_labels\n",
    "                \n",
    "        batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "        test_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "    test_accuracy = test_accuracy/test_data.shape[0]\n",
    "    test_time = time.time() - tic\n",
    "    msg = \"[TST] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, test_accuracy, test_time)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: M_0 = |V| = 944 nodes (160 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 472 nodes (64 added), |E| = 1426 edges\n",
      "Layer 2: M_2 = |V| = 236 nodes (22 added), |E| = 653 edges\n",
      "Layer 3: M_3 = |V| = 118 nodes (6 added), |E| = 318 edges\n",
      "Layer 4: M_4 = |V| = 59 nodes (0 added), |E| = 152 edges\n",
      "WARNING:tensorflow:From <ipython-input-4-6a009cd2e1fd>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data_mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-5-09dd0f8ec1a9>:259: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "#weights in the model: 1991050\n",
      "[TRN] iter = 000, cost = 1.14e+01, |grad| = 2.72e+01 (1.89e+00s)\n",
      "[VAL] iter = 000, acc = 11.36 (1.49e+00s)\n",
      "[TRN] iter = 600, cost = 3.63e+00, |grad| = 1.34e+00 (5.78e-02s)\n",
      "[VAL] iter = 600, acc = 96.92 (1.30e+00s)\n",
      "[TRN] iter = 1200, cost = 3.11e+00, |grad| = 1.03e+00 (5.44e-02s)\n",
      "[VAL] iter = 1200, acc = 97.70 (1.28e+00s)\n",
      "[TRN] iter = 1800, cost = 2.82e+00, |grad| = 8.66e-01 (5.48e-02s)\n",
      "[VAL] iter = 1800, acc = 97.98 (1.29e+00s)\n",
      "[TRN] iter = 2400, cost = 2.56e+00, |grad| = 1.23e+00 (5.53e-02s)\n",
      "[VAL] iter = 2400, acc = 98.42 (1.28e+00s)\n",
      "[TRN] iter = 3000, cost = 2.31e+00, |grad| = 8.89e-01 (5.49e-02s)\n",
      "[VAL] iter = 3000, acc = 98.42 (1.30e+00s)\n",
      "[TRN] iter = 3600, cost = 2.13e+00, |grad| = 1.09e+00 (5.51e-02s)\n",
      "[VAL] iter = 3600, acc = 98.66 (1.28e+00s)\n",
      "[TRN] iter = 4200, cost = 1.89e+00, |grad| = 7.06e-01 (5.46e-02s)\n",
      "[VAL] iter = 4200, acc = 98.62 (1.29e+00s)\n",
      "[TRN] iter = 4800, cost = 1.75e+00, |grad| = 6.32e-01 (5.54e-02s)\n",
      "[VAL] iter = 4800, acc = 98.74 (1.30e+00s)\n",
      "[TRN] iter = 5400, cost = 1.63e+00, |grad| = 4.86e-01 (5.46e-02s)\n",
      "[VAL] iter = 5400, acc = 98.78 (1.28e+00s)\n",
      "[TRN] iter = 6000, cost = 1.51e+00, |grad| = 6.11e-01 (5.60e-02s)\n",
      "[VAL] iter = 6000, acc = 98.76 (1.29e+00s)\n",
      "[TRN] iter = 6600, cost = 1.40e+00, |grad| = 4.16e-01 (5.61e-02s)\n",
      "[VAL] iter = 6600, acc = 98.92 (1.30e+00s)\n",
      "[TRN] iter = 7200, cost = 1.36e+00, |grad| = 1.07e+00 (5.55e-02s)\n",
      "[VAL] iter = 7200, acc = 98.86 (1.34e+00s)\n",
      "[TRN] iter = 7800, cost = 1.23e+00, |grad| = 3.22e-01 (5.56e-02s)\n",
      "[VAL] iter = 7800, acc = 98.92 (1.29e+00s)\n",
      "[TRN] iter = 8400, cost = 1.18e+00, |grad| = 6.71e-01 (5.46e-02s)\n",
      "[VAL] iter = 8400, acc = 99.02 (1.29e+00s)\n",
      "[TRN] iter = 9000, cost = 1.15e+00, |grad| = 7.70e-01 (5.84e-02s)\n",
      "[VAL] iter = 9000, acc = 98.88 (1.29e+00s)\n",
      "[TRN] iter = 9600, cost = 1.08e+00, |grad| = 1.20e+00 (5.46e-02s)\n",
      "[VAL] iter = 9600, acc = 99.00 (1.30e+00s)\n",
      "[TRN] iter = 10200, cost = 1.00e+00, |grad| = 3.76e-01 (5.74e-02s)\n",
      "[VAL] iter = 10200, acc = 98.86 (1.32e+00s)\n",
      "[TRN] iter = 10800, cost = 9.83e-01, |grad| = 9.74e-01 (5.48e-02s)\n",
      "[VAL] iter = 10800, acc = 98.92 (1.31e+00s)\n",
      "[TRN] iter = 11400, cost = 9.24e-01, |grad| = 4.74e-01 (5.46e-02s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e8bb8e70c3b1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m                              learning_obj.ph_dropout: dropout_val_test}\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mbatch_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0mval_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_accuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (GDL_HW1)",
   "language": "python",
   "name": "pycharm-56591012"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
