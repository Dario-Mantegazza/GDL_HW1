{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#import graph, coarsening, utils\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport graph\n",
    "%aimport coarsening\n",
    "%aimport utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import time, shutil\n",
    "import numpy as np\n",
    "import os, collections, sklearn\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Definition of some flags useful later in the code\n",
    "\n",
    "number_edges = 8\n",
    "metric = 'euclidean'\n",
    "normalized_laplacian = True\n",
    "coarsening_levels = 4\n",
    "# Directories.\n",
    "dir_data = 'data_mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n",
    "#for each level\n",
    "\n",
    "def grid_graph(m): # Create the graph as a grid\n",
    "    z = graph.grid(m)  # normalized nodes coordinates\n",
    "    dist, idx_ = graph.distance_sklearn_metrics(z, k=number_edges, metric=metric) \n",
    "    #dist contains the distance of the 8 nearest neighbors for each node indicated in z sorted in ascending order\n",
    "    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n",
    "\n",
    "    A_ = graph.adjacency(dist, idx_)  # graph.adjacency() builds a sparse matrix out of the identified edges computing similarities as: A_{ij} = e^(-dist_{ij}^2/sigma^2)\n",
    "    \n",
    "    return A_, z\n",
    "\n",
    "def coarsen_for_MoNet(A, nodes_coordinates, levels):\n",
    "    graphs, parents = coarsening.metis(A, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n",
    "                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n",
    "                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n",
    "                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n",
    "                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n",
    "                                                  #in the choice of the neighbor of each node.\n",
    "                    \n",
    "                                                  #Construction is done a priori, so we have one graph for all the samples!\n",
    "                    \n",
    "                                                  #graphs = list of spare adjacency matrices (it contains in position \n",
    "                                                  #          0 the original graph)\n",
    "                                                  #parents = list of numpy arrays (every array in position i contains \n",
    "                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n",
    "                                                  #           node i in the coarsed graph -> that is, the idx of its cluster) \n",
    "    perms = coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n",
    "                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n",
    "                                             #to produce the coarsed version of the graph.\n",
    "                                             #Fake nodes are appended for each node which is not grouped with anybody else\n",
    "    coordinates = np.copy(nodes_coordinates)\n",
    "    \n",
    "    u_rows, u_cols, u_values, u_shapes = [], [], [], []\n",
    "    for i,A in enumerate(graphs):\n",
    "        M = A.shape[0]\n",
    "\n",
    "        # We remove self-connections created by metis.\n",
    "        A = A.tocoo()\n",
    "        A.setdiag(0)\n",
    "\n",
    "        if i < levels: #if we have to pool the graph \n",
    "            A = coarsening.perm_adjacency(A, perms[i]) #matrix A is here extended with the fakes nodes\n",
    "                                                       #in order to do an efficient pooling operation\n",
    "                                                       #in tensorflow as it was a 1D pooling\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        \n",
    "        Mnew = A.shape[0]\n",
    "        u_shapes.append(Mnew)          \n",
    "        if i == 0:\n",
    "            # since we use distances the coordinate vec needs to be extended to the fakes node\n",
    "            number_of_fake_node = Mnew-M\n",
    "            coordinates = np.concatenate([coordinates, np.ones([number_of_fake_node, 2])*np.inf], 0)\n",
    "            coordinates = coordinates[perms[0]]       \n",
    "        \n",
    "        print('Layer {0}: U_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n",
    "\n",
    "        from_node, to_node = A.nonzero()\n",
    "        u_rows.append(from_node); u_cols.append(to_node)\n",
    "        \n",
    "        dist_nodes_vec = coordinates[from_node] - coordinates[to_node]\n",
    "        u_values.append(dist_nodes_vec)\n",
    "        \n",
    "        \n",
    "        # update coordinates for next coarser graph\n",
    "        new_coordinates = []\n",
    "        for k in range(A.shape[0]//2):\n",
    "            idx_first_el = k * 2\n",
    "            \n",
    "            if not np.isfinite(coordinates[idx_first_el][0]):\n",
    "                new_coordinates.append(coordinates[idx_first_el+1])\n",
    "                \n",
    "            elif not np.isfinite(coordinates[idx_first_el+1][0]):\n",
    "                new_coordinates.append(coordinates[idx_first_el])\n",
    "                \n",
    "            else:\n",
    "                new_coordinates.append(np.mean(coordinates[idx_first_el:idx_first_el+2], axis=0))\n",
    "                \n",
    "        coordinates = np.asarray(new_coordinates)\n",
    "        \n",
    "    U = []\n",
    "    for it in range(coarsening_levels):\n",
    "        U.append([u_rows[it], u_cols[it], u_values[it], u_shapes[it]])\n",
    "    print(\"shape_u 0:\"+str(len(u_values[0])))\n",
    "    return  U, perms[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    #loading of MNIST dataset\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist_ = input_data.read_data_sets(dir_data, one_hot=False)\n",
    "    train_data_ = mnist_.train.images.astype(np.float32)\n",
    "    val_data_ = mnist_.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n",
    "    #are used for validation\n",
    "    test_data_ = mnist_.test.images.astype(np.float32)\n",
    "    train_labels_ = mnist_.train.labels\n",
    "    val_labels_ = mnist_.validation.labels\n",
    "    test_labels_ = mnist_.test.labels\n",
    "    t_start = time.time()\n",
    "    np.random.seed(0)\n",
    "    n_rows_cols = 28\n",
    "    A_, nodes_coordinates = grid_graph(n_rows_cols)\n",
    "    U, perm_OG = coarsen_for_MoNet(A_, nodes_coordinates, coarsening_levels)\n",
    "    print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "   \n",
    "    train_data_ = coarsening.perm_data(train_data_, perm_OG)\n",
    "    val_data_ = coarsening.perm_data(val_data_, perm_OG)\n",
    "    test_data_ = coarsening.perm_data(test_data_, perm_OG)\n",
    "    del perm_OG\n",
    "    return  U, mnist_, test_data_, test_labels_, train_data_, train_labels_, val_data_, val_labels_, A_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# class ChebNet:\n",
    "class MyMoNet:\n",
    "    \"\"\"\n",
    "    The neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Helper functions used for constructing the model\n",
    "    def _weight_variable(self, shape, regularization=True): \n",
    "        \"\"\"Initializer for the weights\"\"\"\n",
    "        \n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n",
    "        if regularization: #append the loss of the current variable to the regularization term \n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        \"\"\"Initializer for the bias\"\"\"\n",
    "        \n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "\n",
    "    def frobenius_norm(self, tensor): \n",
    "        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n",
    "        \n",
    "        square_tensor = tf.square(tensor)\n",
    "        tensor_sum = tf.reduce_sum(square_tensor)\n",
    "        frobenius_norm = tf.sqrt(tensor_sum)\n",
    "        return frobenius_norm\n",
    "    \n",
    "    \n",
    "    def count_no_weights(self):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        print('#weights in the model: %d' % (total_parameters,))\n",
    "\n",
    "    def patch_operator(self,j,u, x): # D_j(x)f patch operator\n",
    "        N, M, Fin = x.get_shape()\n",
    "        indices_row_, indices_col_, values_u_, shape_u_ = u\n",
    "        with tf.variable_scope('guassian_filter_{}'.format(j)):  #aka the kernel\n",
    "            mu_j = tf.get_variable('mu_{}'.format(j), [1, values_u_.shape[1]], tf.float32)\n",
    "            precision_j = tf.get_variable('precision_{}'.format(j), [1, values_u_.shape[1]], tf.float32, initializer=tf.initializers.random_uniform(minval=0.1,maxval=1))\n",
    "            \n",
    "            j_ = tf.reduce_sum((-0.5)*(values_u_-mu_j)*precision_j*(values_u_-mu_j), axis=-1)\n",
    "   \n",
    "            w_j=  tf.exp(j_, name='w_{}'.format(j))\n",
    "            w_gaussian = tf.SparseTensor(indices=np.vstack([indices_row_, indices_col_]).T, \n",
    "                                        values=w_j, \n",
    "                                        dense_shape=[shape_u_,shape_u_] )\n",
    "        x_t = tf.reshape(tf.transpose(x, [1,2,0]), [M, Fin*N])  \n",
    "        temp__=tf.sparse_tensor_dense_matmul(w_gaussian, x_t)\n",
    "        \n",
    "        return tf.transpose(tf.reshape(temp__, [M, Fin, N]), [2,0,1])\n",
    "    \n",
    "    \n",
    "    def MyMoNetConv(self, x, u, Fout, n_filters): \n",
    "        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n",
    "        N, M, Fin = x.get_shape()  # N is the number of images # batch size \n",
    "                                   # M the number of vertices in the images\n",
    "                                   # Fin the number of features\n",
    "        d_j_list=[x]\n",
    "        for j in range(n_filters): # Cycle through the Gaussian\n",
    "            d_j = self.patch_operator(j,u,x)\n",
    "            d_j_list.append(d_j)\n",
    "        d_j_conv=tf.stack(d_j_list,0)\n",
    "        \n",
    "        d_j_conv = tf.transpose(d_j_conv, [1,2,3,0]) \n",
    "        d_j_conv = tf.reshape(d_j_conv, [N*M, Fin*(n_filters+1)]) \n",
    "        G = self._weight_variable([Fin*(n_filters+1), Fout])\n",
    "        out_conv = tf.matmul(d_j_conv, G) \n",
    "        result = tf.reshape(out_conv, [N, M, Fout])\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n",
    "\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # shape = N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # shape = N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "    \n",
    "    def def_layer_stack(self, i, x):\n",
    "        with tf.variable_scope('layer_stack_{}'.format(i)):\n",
    "            with tf.name_scope('Monet_conv'):\n",
    "                conv_out = self.MyMoNetConv(x,self.u[i*2],self.F[i], self.K[i])\n",
    "            with tf.name_scope('relu'):\n",
    "                relu_out = self.b1relu(conv_out)\n",
    "            with tf.name_scope('pool'):\n",
    "                pool_out = self.mpool1(relu_out, self.p[i])\n",
    "        return pool_out\n",
    "    \n",
    "    #function used for extracting the result of our model\n",
    "    def _inference(self, x, dropout): #definition of the model\n",
    "       # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.def_layer_stack(i,x)\n",
    "        pool_out=x\n",
    "        # Fully connected hidden layers.\n",
    "        N, M, F = pool_out.get_shape()\n",
    "        pool_out_reshaped = tf.reshape(pool_out, [int(N), int(M*F)])  # N x M\n",
    "        for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n",
    "                                           #(we discard the last value in M since it contains the number of classes we have\n",
    "                                           #to predict)\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                fc_out = self.fc(pool_out_reshaped, M)\n",
    "                dropout_out = tf.nn.dropout(fc_out, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            logits = self.fc(dropout_out, self.M[-1], relu=False)\n",
    "        return logits\n",
    "    \n",
    "    def convert_coo_to_sparse_tensor(self, L):\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data.astype('float32'), L.shape)\n",
    "        L = tf.sparse_reorder(L)\n",
    "        return L\n",
    "        \n",
    "    \n",
    "    def __init__(self, p, K, F, M, M_0, batch_size, u ,decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, num_layers= 2,idx_gpu = '/gpu:0'):\n",
    "        self.regularizers = list()  # list of regularization l2 loss for multiple variables\n",
    "        self.num_layers = num_layers\n",
    "        self.p = p                  # dimensions of the pooling layers\n",
    "        self.K = K                  # Number of Gaussian used \n",
    "        self.dim_d = 2              # dimensionality of function u(x,y) with x vertex and y vertex in N(x)\n",
    "        self.F = F                  # Number of features of convolutional layers\n",
    "        self.M = M                  # Number of neurons in fully connected layers\n",
    "        self.M_0 = M_0              # number of elements in the first graph \n",
    "        self.batch_size = batch_size \n",
    "        \n",
    "                        #definition of some learning parameters\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization \n",
    "        with tf.Graph().as_default() as g:\n",
    "                self.graph = g\n",
    "                tf.set_random_seed(0)\n",
    "                with tf.device(idx_gpu):\n",
    "                        self.u = u\n",
    "                        #definition of placeholders\n",
    "                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                        self.ph_labels = tf.placeholder(tf.int32, self.batch_size, 'labels')\n",
    "                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "                    \n",
    "                        #Model construction\n",
    "                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n",
    "                        \n",
    "                        #Definition of the loss function\n",
    "                        with tf.name_scope('loss'):\n",
    "                            print(self.logits)\n",
    "                            print(self.ph_labels)\n",
    "                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, \n",
    "                                                                                                labels=self.ph_labels)\n",
    "                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n",
    "                        with tf.name_scope('regularization'):\n",
    "                            self.regularization *= tf.add_n(self.regularizers)\n",
    "                        self.loss = self.cross_entropy #+ self.regularization\n",
    "                        \n",
    "                        #Solver Definition\n",
    "                        with tf.name_scope('training'):\n",
    "                            # Learning rate.\n",
    "                            global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n",
    "                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n",
    "                                learning_rate = tf.train.exponential_decay(\n",
    "                                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "                            # Optimizer.\n",
    "                            # if momentum == 0:\n",
    "                            #     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                            #  else: #applies momentum for increasing the robustness of the gradient \n",
    "                            #     optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "                            # grads = optimizer.compute_gradients(self.loss)\n",
    "                            # self.op_gradients = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "                            optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=0.8)\n",
    "                            self.op_gradients = optimizer.minimize(self.loss)\n",
    "                            \n",
    "                        #Computation of the norm gradients (useful for debugging)\n",
    "                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n",
    "                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad if g!=None], 0))\n",
    "\n",
    "                        #Extraction of the predictions and computation of accuracy\n",
    "                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n",
    "                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n",
    "        \n",
    "                        # Create a session for running Ops on the Graph.\n",
    "                        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "                        config.gpu_options.allow_growth = True\n",
    "                        self.session = tf.Session(config=config)\n",
    "\n",
    "                        # Run the Op to initialize the variables.\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        self.session.run(init)\n",
    "                        \n",
    "                        self.count_no_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    U, mnist, test_data, test_labels, train_data, train_labels, val_data, val_labels, A = load_mnist()\n",
    "    np.random.seed(0)\n",
    "    #Convolutional parameters\n",
    "    p = [4, 4]   #Dimensions of the pooling layers\n",
    "    K = [25, 25] #List of polynomial orders, i.e. filter sizes or number of hops\n",
    "    F = [32, 64] #Number of features of convolutional layers\n",
    "    \n",
    "    #FC parameters\n",
    "    C = max(mnist.train.labels) + 1 #Number of classes we have\n",
    "    M = [512, C] #Number of neurons in fully connected layers\n",
    "    \n",
    "    #Solver parameters\n",
    "    batch_size = 100\n",
    "    decay_steps = mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n",
    "    decay_rate = 0.95 #how much decreasing the learning rate\n",
    "    learning_rate = 1e-5\n",
    "    momentum = 0.9\n",
    "    regularization = 1e-4\n",
    "    \n",
    "    #Definition of keep probabilities for dropout layers\n",
    "    dropout_training = 0.5\n",
    "    dropout_val_test = 1.0\n",
    "    #%%\n",
    "  \n",
    "    M_0 = U[0][3] #number of elements in the first graph\n",
    "    print(\"M_0: \"+str(M_0))\n",
    "    learning_obj = MyMoNet(p, K, F, M, M_0, batch_size, U, decay_steps, decay_rate, learning_rate=learning_rate, regularization=regularization,momentum=momentum)\n",
    "    # learning_obj = MyMoNet(p, K, F, M, M_0, batch_size, L,u ,decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, num_layers= 2,idx_gpu = '/gpu:0')\n",
    "    \n",
    "    #definition of overall number of training iterations and validation frequency\n",
    "    num_iter_val = 100\n",
    "    num_total_iter_training = 2100\n",
    "    \n",
    "    num_iter = 0\n",
    "    \n",
    "    list_training_loss = list()\n",
    "    list_training_norm_grad = list()\n",
    "    list_val_accuracy = list()\n",
    "    #%%\n",
    "    #training and validation\n",
    "    indices = collections.deque() #queue that will contain a permutation of the training indexes\n",
    "    for k in range(num_iter, num_total_iter_training):\n",
    "        \n",
    "        #Construction of the training batch\n",
    "        if len(indices) < batch_size: # Be sure to have used all the samples before using one a second time.\n",
    "            indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n",
    "        idx = [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n",
    "        \n",
    "        #data extraction\n",
    "        batch_data, batch_labels = train_data[idx,:], train_labels[idx] \n",
    "        \n",
    "        feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                     learning_obj.ph_labels: batch_labels, \n",
    "                     learning_obj.ph_dropout: dropout_training}\n",
    "        \n",
    "        #Training\n",
    "        tic = time.time()\n",
    "        _, current_training_loss, norm_grad, logii = learning_obj.session.run([learning_obj.op_gradients, \n",
    "                                                                        learning_obj.loss, \n",
    "                                                                        learning_obj.norm_grad, learning_obj.logits], feed_dict = feed_dict) \n",
    "        training_time = time.time() - tic\n",
    "        \n",
    "        list_training_loss.append(current_training_loss)\n",
    "        list_training_norm_grad.append(norm_grad)\n",
    "        #print(np.any(np.isnan(logii)))\n",
    "        if (np.mod(num_iter, num_iter_val)==0): #validation\n",
    "            msg = \"[TRN] iter = %03i, cost = %3.2e, |grad| = %.2e (%3.2es)\" \\\n",
    "                        % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n",
    "            print(msg)\n",
    "            \n",
    "            #Validation Code\n",
    "            tic = time.time()\n",
    "            val_accuracy = 0\n",
    "            for begin in range(0, val_data.shape[0], batch_size):\n",
    "                end = begin + batch_size\n",
    "                end = min([end, val_data.shape[0]])\n",
    "                \n",
    "                #data extraction\n",
    "                batch_data = np.zeros((end-begin, val_data.shape[1]))\n",
    "                batch_data = val_data[begin:end,:]\n",
    "                batch_labels = np.zeros(batch_size)\n",
    "                batch_labels[:end-begin] = val_labels[begin:end]\n",
    "                \n",
    "                feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                             learning_obj.ph_labels: batch_labels,\n",
    "                             learning_obj.ph_dropout: dropout_val_test}\n",
    "                \n",
    "                batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "                val_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "            val_accuracy = val_accuracy/val_data.shape[0]\n",
    "            val_time = time.time() - tic\n",
    "            msg = \"[VAL] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n",
    "            print(msg)\n",
    "        num_iter += 1\n",
    "    #Test code\n",
    "    tic = time.time()\n",
    "    test_accuracy = 0\n",
    "    for begin in range(0, test_data.shape[0], batch_size):\n",
    "        end = begin + batch_size\n",
    "        end = min([end, test_data.shape[0]])\n",
    "                \n",
    "        batch_data = np.zeros((end-begin, test_data.shape[1]))\n",
    "        batch_data = test_data[begin:end,:]\n",
    "                \n",
    "        feed_dict = {learning_obj.ph_data: batch_data, learning_obj.ph_dropout: 1}\n",
    "                \n",
    "        batch_labels = np.zeros(batch_size)\n",
    "        batch_labels[:end-begin] = test_labels[begin:end]\n",
    "        feed_dict[learning_obj.ph_labels] = batch_labels\n",
    "                \n",
    "        batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "        test_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "    test_accuracy = test_accuracy/test_data.shape[0]\n",
    "    test_time = time.time() - tic\n",
    "    msg = \"[TST] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, test_accuracy, test_time)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n",
      "Layer 0: U_0 = |V| = 944 nodes (160 added), |E| = 3198 edges\n",
      "Layer 1: U_1 = |V| = 472 nodes (64 added), |E| = 1426 edges\n",
      "Layer 2: U_2 = |V| = 236 nodes (22 added), |E| = 653 edges\n",
      "Layer 3: U_3 = |V| = 118 nodes (6 added), |E| = 318 edges\n",
      "Layer 4: U_4 = |V| = 59 nodes (0 added), |E| = 152 edges\n",
      "shape_u 0:6396\n",
      "Execution time: 0.37s\n",
      "M_0: 944\n",
      "Tensor(\"logits/add:0\", shape=(100, 10), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"labels:0\", shape=(100,), dtype=int32, device=/device:GPU:0)\n",
      "#weights in the model: 1993330\n",
      "[TRN] iter = 000, cost = 8.64e+01, |grad| = 5.83e+02 (3.70e+00s)\n",
      "[VAL] iter = 000, acc = 8.40 (1.02e+00s)\n",
      "[TRN] iter = 100, cost = 9.84e+00, |grad| = 9.87e+01 (7.07e-02s)\n",
      "[VAL] iter = 100, acc = 53.64 (7.35e-01s)\n",
      "[TRN] iter = 200, cost = 1.47e+00, |grad| = 3.96e+01 (7.07e-02s)\n",
      "[VAL] iter = 200, acc = 76.42 (7.57e-01s)\n",
      "[TRN] iter = 300, cost = 9.92e-01, |grad| = 4.49e+01 (7.08e-02s)\n",
      "[VAL] iter = 300, acc = 82.12 (7.47e-01s)\n",
      "[TRN] iter = 400, cost = 9.47e-01, |grad| = 4.02e+01 (7.10e-02s)\n",
      "[VAL] iter = 400, acc = 84.00 (7.33e-01s)\n",
      "[TRN] iter = 500, cost = 8.97e-01, |grad| = 3.86e+01 (7.07e-02s)\n",
      "[VAL] iter = 500, acc = 86.58 (7.32e-01s)\n",
      "[TRN] iter = 600, cost = 6.44e-01, |grad| = 2.17e+01 (7.09e-02s)\n",
      "[VAL] iter = 600, acc = 87.42 (7.40e-01s)\n",
      "[TRN] iter = 700, cost = 5.44e-01, |grad| = 1.66e+01 (7.10e-02s)\n",
      "[VAL] iter = 700, acc = 89.02 (7.53e-01s)\n",
      "[TRN] iter = 800, cost = 4.66e-01, |grad| = 2.51e+01 (7.12e-02s)\n",
      "[VAL] iter = 800, acc = 89.52 (7.24e-01s)\n",
      "[TRN] iter = 900, cost = 4.86e-01, |grad| = 2.37e+01 (7.09e-02s)\n",
      "[VAL] iter = 900, acc = 89.58 (7.42e-01s)\n",
      "[TRN] iter = 1000, cost = 5.55e-01, |grad| = 2.51e+01 (7.08e-02s)\n",
      "[VAL] iter = 1000, acc = 91.00 (7.30e-01s)\n",
      "[TRN] iter = 1100, cost = 6.75e-01, |grad| = 3.41e+01 (7.44e-02s)\n",
      "[VAL] iter = 1100, acc = 90.90 (7.33e-01s)\n",
      "[TRN] iter = 1200, cost = 4.93e-01, |grad| = 2.02e+01 (7.10e-02s)\n",
      "[VAL] iter = 1200, acc = 91.32 (7.34e-01s)\n",
      "[TRN] iter = 1300, cost = 3.81e-01, |grad| = 2.43e+01 (7.59e-02s)\n",
      "[VAL] iter = 1300, acc = 91.70 (7.17e-01s)\n",
      "[TRN] iter = 1400, cost = 3.60e-01, |grad| = 1.88e+01 (7.10e-02s)\n",
      "[VAL] iter = 1400, acc = 92.26 (7.42e-01s)\n",
      "[TRN] iter = 1500, cost = 3.27e-01, |grad| = 2.98e+01 (7.21e-02s)\n",
      "[VAL] iter = 1500, acc = 92.08 (7.50e-01s)\n",
      "[TRN] iter = 1600, cost = 3.44e-01, |grad| = 1.24e+01 (7.27e-02s)\n",
      "[VAL] iter = 1600, acc = 92.54 (7.36e-01s)\n",
      "[TRN] iter = 1700, cost = 3.03e-01, |grad| = 1.58e+01 (7.09e-02s)\n",
      "[VAL] iter = 1700, acc = 92.82 (7.25e-01s)\n",
      "[TRN] iter = 1800, cost = 4.60e-01, |grad| = 1.79e+01 (7.11e-02s)\n",
      "[VAL] iter = 1800, acc = 92.82 (7.32e-01s)\n",
      "[TRN] iter = 1900, cost = 4.83e-01, |grad| = 1.84e+01 (7.10e-02s)\n",
      "[VAL] iter = 1900, acc = 93.30 (7.47e-01s)\n",
      "[TRN] iter = 2000, cost = 3.77e-01, |grad| = 2.05e+01 (7.12e-02s)\n",
      "[VAL] iter = 2000, acc = 93.34 (7.52e-01s)\n",
      "[TST] iter = 2100, acc = 92.15 (1.51e+00s)\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (GDL_HW1)",
   "language": "python",
   "name": "pycharm-56591012"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
