{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "#import graph, coarsening, utils\n",
        "%load_ext autoreload\n",
        "%autoreload 1\n",
        "%aimport graph\n",
        "%aimport coarsening\n",
        "%aimport utils\n",
        "\n",
        "import tensorflow as tf\n",
        "import time, shutil\n",
        "import numpy as np\n",
        "import os, collections, sklearn\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "#Definition of some flags useful later in the code\n\nnumber_edges \u003d 8\nmetric \u003d \u0027euclidean\u0027\nnormalized_laplacian \u003d True\ncoarsening_levels \u003d 4\n# Directories.\ndir_data \u003d \u0027data_mnist\u0027\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": false,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "#Here we proceed at computing the original grid where the images live and the various coarsening that are applied\n#for each level\n\ndef grid_graph(m):\n    z \u003d graph.grid(m)  # normalized nodes coordinates\n    dist, idx_ \u003d graph.distance_sklearn_metrics(z, k\u003dnumber_edges, metric\u003dmetric) \n    #dist contains the distance of the 8 nearest neighbors for each node indicated in z sorted in ascending order\n    #idx contains the indexes of the 8 nearest for each node sorted in ascending order by distance\n\n    A_ \u003d graph.adjacency(dist, idx_)  # graph.adjacency() builds a sparse matrix out of the identified edges computing similarities as: A_{ij} \u003d e^(-dist_{ij}^2/sigma^2)\n    \n    return A_, z\n\ndef coarsen(A_, levels): # Equivalent to the pooling layer\n    graphs, parents \u003d coarsening.metis(A_, levels) #Coarsen a graph multiple times using Graclus variation of the METIS algorithm. \n                                                  #Basically, we randomly sort the nodes, we iterate on them and we decided to group each node\n                                                  #with the neighbor having highest w_ij * 1/(\\sum_k w_ik) + w_ij * 1/(\\sum_k w_kj) \n                                                  #i.e. highest sum of probabilities to randomly walk from i to j and from j to i.\n                                                  #We thus favour strong connections (i.e. the ones with high weight wrt all the others for both nodes) \n                                                  #in the choice of the neighbor of each node.\n                    \n                                                  #Construction is done a priori, so we have one graph for all the samples!\n                    \n                                                  #graphs \u003d list of spare adjacency matrices (it contains in position \n                                                  #          0 the original graph)\n                                                  #parents \u003d list of numpy arrays (every array in position i contains \n                                                  #           the mapping from graph i to graph i+1, i.e. the idx of\n                                                  #           node i in the coarsed graph -\u003e that is, the idx of its cluster) \n    perms \u003d coarsening.compute_perm(parents) #Return a list of indices to reorder the adjacency and data matrices so\n                                             #that two consecutive nodes correspond to neighbors that should be collapsed\n                                             #to produce the coarsed version of the graph.\n                                             #Fake nodes are appended for each node which is not grouped with anybody else\n    laplacians \u003d []\n    for i,A__ in enumerate(graphs):\n        M_, M_ \u003d A__.shape\n\n        # We remove self-connections created by metis.\n        A__ \u003d A__.tocoo()\n        A__.setdiag(0)\n\n        if i \u003c levels: #if we have to pool the graph \n            A__ \u003d coarsening.perm_adjacency(A__, perms[i]) #matrix A is here extended with the fakes nodes\n                                                       #in order to do an efficient pooling operation\n                                                       #in tensorflow as it was a 1D pooling\n\n        A__ \u003d A__.tocsr()\n        A__.eliminate_zeros()\n        Mnew, Mnew \u003d A__.shape\n        print(\u0027Layer {0}: M_{0} \u003d |V| \u003d {1} nodes ({2} added), |E| \u003d {3} edges\u0027.format(i, Mnew, Mnew-M_, A__.nnz//2))\n\n        L_ \u003d graph.laplacian(A__, normalized\u003dnormalized_laplacian)\n        laplacians.append(L_)\n\n    return laplacians, perms[0] if len(perms) \u003e 0 else None"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data_mnist/train-images-idx3-ubyte.gz\n",
            "Extracting data_mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting data_mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting data_mnist/t10k-labels-idx1-ubyte.gz\n",
            "Execution time: 8.46s\n"
          ]
        }
      ],
      "source": "#loading of MNIST dataset\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist \u003d input_data.read_data_sets(dir_data, one_hot\u003dFalse)\n\ntrain_data \u003d mnist.train.images.astype(np.float32)\nval_data \u003d mnist.validation.images.astype(np.float32) #the first 5K samples of the training dataset \n                                                      #are used for validation\ntest_data \u003d mnist.test.images.astype(np.float32)\ntrain_labels \u003d mnist.train.labels\nval_labels \u003d mnist.validation.labels\ntest_labels \u003d mnist.test.labels\n\nt_start \u003d time.time()\ntrain_data \u003d coarsening.perm_data(train_data, perm)\nval_data \u003d coarsening.perm_data(val_data, perm)\ntest_data \u003d coarsening.perm_data(test_data, perm)\nprint(\u0027Execution time: {:.2f}s\u0027.format(time.time() - t_start))\ndel perm"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "class ChebNet:\n",
        "    \"\"\"\n",
        "    The neural network model.\n",
        "    \"\"\"\n",
        "    \n",
        "    #Helper functions used for constructing the model\n",
        "    def _weight_variable(self, shape, regularization\u003dTrue): \n",
        "        \"\"\"Initializer for the weights\"\"\"\n",
        "        \n",
        "        initial \u003d tf.truncated_normal_initializer(0, 0.1)\n",
        "        var \u003d tf.get_variable(\u0027weights\u0027, shape, tf.float32, initializer\u003dinitial)\n",
        "        if regularization: #append the loss of the current variable to the regularization term \n",
        "            self.regularizers.append(tf.nn.l2_loss(var))\n",
        "        return var\n",
        "    \n",
        "    def _bias_variable(self, shape, regularization\u003dTrue):\n",
        "        \"\"\"Initializer for the bias\"\"\"\n",
        "        \n",
        "        initial \u003d tf.constant_initializer(0.1)\n",
        "        var \u003d tf.get_variable(\u0027bias\u0027, shape, tf.float32, initializer\u003dinitial)\n",
        "        if regularization:\n",
        "            self.regularizers.append(tf.nn.l2_loss(var))\n",
        "        return var\n",
        "    \n",
        "\n",
        "    def frobenius_norm(self, tensor): \n",
        "        \"\"\"Computes the frobenius norm for a given tensor\"\"\"\n",
        "        \n",
        "        square_tensor \u003d tf.square(tensor)\n",
        "        tensor_sum \u003d tf.reduce_sum(square_tensor)\n",
        "        frobenius_norm \u003d tf.sqrt(tensor_sum)\n",
        "        return frobenius_norm\n",
        "    \n",
        "    \n",
        "    def count_no_weights(self):\n",
        "        total_parameters \u003d 0\n",
        "        for variable in tf.trainable_variables():\n",
        "            # shape is an array of tf.Dimension\n",
        "            shape \u003d variable.get_shape()\n",
        "            variable_parameters \u003d 1\n",
        "            for dim in shape:\n",
        "                variable_parameters *\u003d dim.value\n",
        "            total_parameters +\u003d variable_parameters\n",
        "        print(\u0027#weights in the model: %d\u0027 % (total_parameters,))\n",
        "\n",
        "    \n",
        "    #Modules used by the graph convolutional network\n",
        "    def chebyshevConv(self, x, L, Fout, K): \n",
        "        \"\"\"Applies chebyshev polynomials over the graph (i.e. it makes a spectral convolution)\"\"\"\n",
        "        \n",
        "        N, M, Fin \u003d x.get_shape()  # N is the number of images\n",
        "                                   # M the number of vertices in the images\n",
        "                                   # Fin the number of features\n",
        "        N, M, Fin \u003d int(N), int(M), int(Fin)\n",
        "        \n",
        "        # Transform to Chebyshev basis. Here we apply the chebyshev polynomials. For simplicity instead of operating\n",
        "        # over the eigenvalues of the laplacians we apply directly the chebysev polynomials to the laplacians.\n",
        "        # This makes everything more efficient if the laplacians are actually represented as sparse matrices.\n",
        "        x0 \u003d x # L^0 * x \u003d I * x\n",
        "        x \u003d tf.expand_dims(x0, 0)  # shape \u003d 1 x N x M x Fin\n",
        "        def concat(x, x_):\n",
        "            x_ \u003d tf.expand_dims(x_, 0)  # shape \u003d 1 x M x Fin*N\n",
        "            return tf.concat([x, x_], 0)  # shape \u003d 1 x N x M x Fin\n",
        "        if K \u003e 1:\n",
        "            x0 \u003d tf.transpose(x0, [1,2,0])\n",
        "            x0 \u003d tf.reshape(x0, [M, Fin*N])\n",
        "            x1 \u003d tf.sparse_tensor_dense_matmul(L, x0) #shape \u003d M x Fin*N \u003c- L^1 * x\n",
        "            x1 \u003d tf.reshape(x1, [M, Fin, N]) # shape \u003d M x Fin x N \u003c- L^1 * x\n",
        "            x1 \u003d tf.transpose(x1, [2,0,1]) # shape \u003d N x M x Fin\n",
        "            x0 \u003d tf.reshape(x0, [M, Fin, N])\n",
        "            x0 \u003d tf.transpose(x0, [2,0,1]) # shape \u003d N x M x Fin\n",
        "            x \u003d concat(x, x1) # shape \u003d 2 x N x M x Fin\n",
        "        for k in range(2, K):\n",
        "            x1 \u003d tf.transpose(x1, [1,2,0])\n",
        "            x1 \u003d tf.reshape(x1, [M, Fin*N])\n",
        "            x2 \u003d tf.sparse_tensor_dense_matmul(L, x1) # shape \u003d M x Fin*N \u003c- L^k * x\n",
        "            x2 \u003d tf.reshape(x2, [M, Fin, N]) # shape \u003d M x Fin x N \u003c- L^1 * x\n",
        "            x2 \u003d tf.transpose(x2, [2,0,1])\n",
        "            x1 \u003d tf.reshape(x1, [M, Fin, N])\n",
        "            x1 \u003d tf.transpose(x1, [2,0,1]) # shape \u003d N x M x Fin\n",
        "            x2 \u003d 2 * x2 - x0  # shape \u003d N x M x Fin \u003c- recursive definition of chebyshev polynomials\n",
        "            x \u003d concat(x, x2) # shape \u003d K x N x M x Fin\n",
        "            x0, x1 \u003d x1, x2\n",
        "        x \u003d tf.transpose(x, [1,2,3,0]) # shape \u003d N x M x Fin x K\n",
        "        x \u003d tf.reshape(x, [N*M, Fin*K])  # shape \u003d N*M x Fin*K\n",
        "        \n",
        "        # Filter: Fout filters of order K applied over all the Fin features\n",
        "        W \u003d self._weight_variable([Fin*K, Fout], regularization\u003dFalse)\n",
        "        x \u003d tf.matmul(x, W)  # N*M x Fout\n",
        "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
        "\n",
        "    def b1relu(self, x):\n",
        "        \"\"\"Applies bias and ReLU. One bias per filter.\"\"\"\n",
        "        N, M, F \u003d x.get_shape()\n",
        "        b \u003d self._bias_variable([1, 1, int(F)], regularization\u003dFalse)\n",
        "        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n",
        "\n",
        "\n",
        "    def mpool1(self, x, p):\n",
        "        \"\"\"Max pooling of size p. Should be a power of 2 (this is possible thanks to the reordering we previously did).\"\"\"\n",
        "        if p \u003e 1:\n",
        "            x \u003d tf.expand_dims(x, 3)  # shape \u003d N x M x F x 1\n",
        "            x \u003d tf.nn.max_pool(x, ksize\u003d[1,p,1,1], strides\u003d[1,p,1,1], padding\u003d\u0027SAME\u0027)\n",
        "            return tf.squeeze(x, [3])  # shape \u003d N x M/p x F\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def fc(self, x, Mout, relu\u003dTrue):\n",
        "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
        "        N, Min \u003d x.get_shape()\n",
        "        W \u003d self._weight_variable([int(Min), Mout], regularization\u003dTrue)\n",
        "        b \u003d self._bias_variable([Mout], regularization\u003dTrue)\n",
        "        x \u003d tf.matmul(x, W) + b\n",
        "        return tf.nn.relu(x) if relu else x\n",
        "    \n",
        "    #function used for extracting the result of our model\n",
        "    def _inference(self, x, dropout): #definition of the model\n",
        "        \n",
        "        # Graph convolutional layers.\n",
        "        x \u003d tf.expand_dims(x, 2)  # N x M x F\u003d1\n",
        "        for i in range(len(self.p)):\n",
        "            with tf.variable_scope(\u0027cgconv{}\u0027.format(i+1)):\n",
        "                with tf.name_scope(\u0027filter\u0027):\n",
        "                    x \u003d self.chebyshevConv(x, self.L[i*2], self.F[i], self.K[i])\n",
        "                with tf.name_scope(\u0027bias_relu\u0027):\n",
        "                    x \u003d self.b1relu(x)\n",
        "                with tf.name_scope(\u0027pooling\u0027):\n",
        "                    x \u003d self.mpool1(x, self.p[i])\n",
        "         \n",
        "        # Fully connected hidden layers.\n",
        "        N, M, F \u003d x.get_shape()\n",
        "        x \u003d tf.reshape(x, [int(N), int(M*F)])  # N x M\n",
        "        for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n",
        "                                           #(we discard the last value in M since it contains the number of classes we have\n",
        "                                           #to predict)\n",
        "            with tf.variable_scope(\u0027fc{}\u0027.format(i+1)):\n",
        "                x \u003d self.fc(x, M)\n",
        "                x \u003d tf.nn.dropout(x, dropout)\n",
        "        \n",
        "        # Logits linear layer, i.e. softmax without normalization.\n",
        "        with tf.variable_scope(\u0027logits\u0027):\n",
        "            x \u003d self.fc(x, self.M[-1], relu\u003dFalse)\n",
        "        return x\n",
        "    \n",
        "    def convert_coo_to_sparse_tensor(self, L):\n",
        "        indices \u003d np.column_stack((L.row, L.col))\n",
        "        L \u003d tf.SparseTensor(indices, L.data.astype(\u0027float32\u0027), L.shape)\n",
        "        L \u003d tf.sparse_reorder(L)\n",
        "        return L\n",
        "        \n",
        "    \n",
        "    def __init__(self, p, K, F, M, M_0, batch_size, L,\n",
        "                 decay_steps, decay_rate, learning_rate\u003d1e-4, momentum\u003d0.9, regularization\u003d5e-4, \n",
        "                 idx_gpu \u003d \u0027/gpu:0\u0027):\n",
        "        self.regularizers \u003d list() #list of regularization l2 loss for multiple variables\n",
        "        \n",
        "        self.p \u003d p #dimensions of the pooling layers\n",
        "        self.K \u003d K #List of polynomial orders, i.e. filter sizes or number of hops\n",
        "        self.F \u003d F #Number of features of convolutional layers\n",
        "        \n",
        "        self.M \u003d M #Number of neurons in fully connected layers\n",
        "        \n",
        "        self.M_0 \u003d M_0 #number of elements in the first graph \n",
        "        \n",
        "        self.batch_size \u003d batch_size\n",
        "        \n",
        "        #definition of some learning parameters\n",
        "        self.decay_steps \u003d decay_steps\n",
        "        self.decay_rate \u003d decay_rate\n",
        "        self.learning_rate \u003d learning_rate\n",
        "        self.regularization \u003d regularization\n",
        "        \n",
        "        with tf.Graph().as_default() as g:\n",
        "                self.graph \u003d g\n",
        "                tf.set_random_seed(0)\n",
        "                with tf.device(idx_gpu):\n",
        "                        #definition of placeholders\n",
        "                        self.L \u003d [self.convert_coo_to_sparse_tensor(c_L.tocoo()) for c_L in L]\n",
        "                        self.ph_data \u003d tf.placeholder(tf.float32, (self.batch_size, M_0), \u0027data\u0027)\n",
        "                        self.ph_labels \u003d tf.placeholder(tf.int32, (self.batch_size), \u0027labels\u0027)\n",
        "                        self.ph_dropout \u003d tf.placeholder(tf.float32, (), \u0027dropout\u0027)\n",
        "                    \n",
        "                        #Model construction\n",
        "                        self.logits \u003d self._inference(self.ph_data, self.ph_dropout)\n",
        "                        \n",
        "                        #Definition of the loss function\n",
        "                        with tf.name_scope(\u0027loss\u0027):\n",
        "                            self.cross_entropy \u003d tf.nn.sparse_softmax_cross_entropy_with_logits(logits\u003dself.logits, \n",
        "                                                                                                labels\u003dself.ph_labels)\n",
        "                            self.cross_entropy \u003d tf.reduce_mean(self.cross_entropy)\n",
        "                        with tf.name_scope(\u0027regularization\u0027):\n",
        "                            self.regularization *\u003d tf.add_n(self.regularizers)\n",
        "                        self.loss \u003d self.cross_entropy + self.regularization\n",
        "                        \n",
        "                        #Solver Definition\n",
        "                        with tf.name_scope(\u0027training\u0027):\n",
        "                            # Learning rate.\n",
        "                            global_step \u003d tf.Variable(0, name\u003d\u0027global_step\u0027, trainable\u003dFalse) #used for counting how many iterations we have done\n",
        "                            if decay_rate !\u003d 1: #applies an exponential decay of the lr wrt the number of iterations done\n",
        "                                learning_rate \u003d tf.train.exponential_decay(\n",
        "                                        learning_rate, global_step, decay_steps, decay_rate, staircase\u003dTrue)\n",
        "                            # Optimizer.\n",
        "                            if momentum \u003d\u003d 0:\n",
        "                                optimizer \u003d tf.train.GradientDescentOptimizer(learning_rate)\n",
        "                            else: #applies momentum for increasing the robustness of the gradient \n",
        "                                optimizer \u003d tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "                            grads \u003d optimizer.compute_gradients(self.loss)\n",
        "                            self.op_gradients \u003d optimizer.apply_gradients(grads, global_step\u003dglobal_step) \n",
        "                            \n",
        "                        #Computation of the norm gradients (useful for debugging)\n",
        "                        self.var_grad \u003d tf.gradients(self.loss, tf.trainable_variables())\n",
        "                        self.norm_grad \u003d self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n",
        "\n",
        "                        #Extraction of the predictions and computation of accuracy\n",
        "                        self.predictions \u003d tf.cast(tf.argmax(self.logits, dimension\u003d1), tf.int32)\n",
        "                        self.accuracy \u003d 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n",
        "        \n",
        "                        # Create a session for running Ops on the Graph.\n",
        "                        config \u003d tf.ConfigProto(allow_soft_placement \u003d True)\n",
        "                        config.gpu_options.allow_growth \u003d True\n",
        "                        self.session \u003d tf.Session(config\u003dconfig)\n",
        "\n",
        "                        # Run the Op to initialize the variables.\n",
        "                        init \u003d tf.global_variables_initializer()\n",
        "                        self.session.run(init)\n",
        "                        \n",
        "                        self.count_no_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#Convolutional parameters\n",
        "p \u003d [4, 4]   #Dimensions of the pooling layers\n",
        "K \u003d [25, 25] #List of polynomial orders, i.e. filter sizes or number of hops\n",
        "F \u003d [32, 64] #Number of features of convolutional layers\n",
        "\n",
        "#FC parameters\n",
        "C \u003d max(mnist.train.labels) + 1 #Number of classes we have\n",
        "M \u003d [512, C] #Number of neurons in fully connected layers\n",
        "\n",
        "#Solver parameters\n",
        "batch_size \u003d 100\n",
        "decay_steps \u003d mnist.train.num_examples / batch_size #number of steps to do before decreasing the learning rate\n",
        "decay_rate \u003d 0.95 #how much decreasing the learning rate\n",
        "learning_rate \u003d 0.02\n",
        "momentum \u003d 0.9\n",
        "regularization \u003d 5e-4\n",
        "\n",
        "#Definition of keep probabilities for dropout layers\n",
        "dropout_training \u003d 0.5\n",
        "dropout_val_test \u003d 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From \u003cipython-input-7-07ffa1c7ed3e\u003e:215: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the `axis` argument instead\n",
            "#weights in the model: 2154890\n"
          ]
        }
      ],
      "source": [
        "#Construction of the learning obj\n",
        "M_0 \u003d L[0].shape[0] #number of elements in the first graph\n",
        "learning_obj \u003d ChebNet(p, K, F, M, M_0, batch_size, L_norm,\n",
        "                       decay_steps, decay_rate,\n",
        "                       learning_rate\u003dlearning_rate, regularization\u003dregularization,\n",
        "                       momentum\u003dmomentum)\n",
        "\n",
        "#definition of overall number of training iterations and validation frequency\n",
        "num_iter_val \u003d 600\n",
        "num_total_iter_training \u003d 21000\n",
        "\n",
        "num_iter \u003d 0\n",
        "\n",
        "list_training_loss \u003d list()\n",
        "list_training_norm_grad \u003d list()\n",
        "list_val_accuracy \u003d list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRN] iter \u003d 000, cost \u003d 1.41e+01, |grad| \u003d 4.02e+01 (3.47e-01s)\n",
            "[VAL] iter \u003d 000, acc \u003d 9.58 (1.37e+00s)\n",
            "[TRN] iter \u003d 600, cost \u003d 3.77e+00, |grad| \u003d 1.07e+00 (1.12e-01s)\n",
            "[VAL] iter \u003d 600, acc \u003d 96.78 (1.34e+00s)\n",
            "[TRN] iter \u003d 1200, cost \u003d 3.33e+00, |grad| \u003d 9.70e-01 (1.07e-01s)\n",
            "[VAL] iter \u003d 1200, acc \u003d 97.68 (1.32e+00s)\n",
            "[TRN] iter \u003d 1800, cost \u003d 3.02e+00, |grad| \u003d 1.01e+00 (1.08e-01s)\n",
            "[VAL] iter \u003d 1800, acc \u003d 98.06 (1.32e+00s)\n",
            "[TRN] iter \u003d 2400, cost \u003d 2.66e+00, |grad| \u003d 3.49e-01 (1.06e-01s)\n",
            "[VAL] iter \u003d 2400, acc \u003d 97.96 (1.31e+00s)\n",
            "[TRN] iter \u003d 3000, cost \u003d 2.46e+00, |grad| \u003d 7.49e-01 (1.06e-01s)\n",
            "[VAL] iter \u003d 3000, acc \u003d 98.20 (1.32e+00s)\n",
            "[TRN] iter \u003d 3600, cost \u003d 2.29e+00, |grad| \u003d 8.09e-01 (1.06e-01s)\n",
            "[VAL] iter \u003d 3600, acc \u003d 98.26 (1.31e+00s)\n",
            "[TRN] iter \u003d 4200, cost \u003d 2.08e+00, |grad| \u003d 7.89e-01 (1.06e-01s)\n",
            "[VAL] iter \u003d 4200, acc \u003d 98.50 (1.31e+00s)\n",
            "[TRN] iter \u003d 4800, cost \u003d 1.89e+00, |grad| \u003d 5.38e-01 (1.05e-01s)\n",
            "[VAL] iter \u003d 4800, acc \u003d 98.46 (1.29e+00s)\n",
            "[TRN] iter \u003d 5400, cost \u003d 1.74e+00, |grad| \u003d 2.88e-01 (1.05e-01s)\n",
            "[VAL] iter \u003d 5400, acc \u003d 98.62 (1.31e+00s)\n",
            "[TRN] iter \u003d 6000, cost \u003d 1.63e+00, |grad| \u003d 3.38e-01 (1.05e-01s)\n",
            "[VAL] iter \u003d 6000, acc \u003d 98.60 (1.31e+00s)\n",
            "[TRN] iter \u003d 6600, cost \u003d 1.52e+00, |grad| \u003d 4.90e-01 (1.06e-01s)\n",
            "[VAL] iter \u003d 6600, acc \u003d 98.66 (1.32e+00s)\n"
          ]
        }
      ],
      "source": [
        "#training and validation\n",
        "indices \u003d collections.deque() #queue that will contain a permutation of the training indexes\n",
        "for k in range(num_iter, num_total_iter_training):\n",
        "    \n",
        "    #Construction of the training batch\n",
        "    if len(indices) \u003c batch_size: # Be sure to have used all the samples before using one a second time.\n",
        "        indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n",
        "    idx \u003d [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n",
        "    \n",
        "    #data extraction\n",
        "    batch_data, batch_labels \u003d train_data[idx,:], train_labels[idx] \n",
        "    \n",
        "    feed_dict \u003d {learning_obj.ph_data: batch_data, \n",
        "                 learning_obj.ph_labels: batch_labels, \n",
        "                 learning_obj.ph_dropout: dropout_training}\n",
        "    \n",
        "    #Training\n",
        "    tic \u003d time.time()\n",
        "    _, current_training_loss, norm_grad \u003d learning_obj.session.run([learning_obj.op_gradients, \n",
        "                                                                    learning_obj.loss, \n",
        "                                                                    learning_obj.norm_grad], feed_dict \u003d feed_dict) \n",
        "    training_time \u003d time.time() - tic\n",
        "    \n",
        "    list_training_loss.append(current_training_loss)\n",
        "    list_training_norm_grad.append(norm_grad)\n",
        "    \n",
        "    if (np.mod(num_iter, num_iter_val)\u003d\u003d0): #validation\n",
        "        msg \u003d \"[TRN] iter \u003d %03i, cost \u003d %3.2e, |grad| \u003d %.2e (%3.2es)\" \\\n",
        "                    % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n",
        "        print(msg)\n",
        "        \n",
        "        #Validation Code\n",
        "        tic \u003d time.time()\n",
        "        val_accuracy \u003d 0\n",
        "        for begin in range(0, val_data.shape[0], batch_size):\n",
        "            end \u003d begin + batch_size\n",
        "            end \u003d min([end, val_data.shape[0]])\n",
        "            \n",
        "            #data extraction\n",
        "            batch_data \u003d np.zeros((end-begin, val_data.shape[1]))\n",
        "            batch_data \u003d val_data[begin:end,:]\n",
        "            batch_labels \u003d np.zeros(batch_size)\n",
        "            batch_labels[:end-begin] \u003d val_labels[begin:end]\n",
        "            \n",
        "            feed_dict \u003d {learning_obj.ph_data: batch_data, \n",
        "                         learning_obj.ph_labels: batch_labels,\n",
        "                         learning_obj.ph_dropout: dropout_val_test}\n",
        "            \n",
        "            batch_accuracy \u003d learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
        "            val_accuracy +\u003d batch_accuracy*batch_data.shape[0]\n",
        "        val_accuracy \u003d val_accuracy/val_data.shape[0]\n",
        "        val_time \u003d time.time() - tic\n",
        "        msg \u003d \"[VAL] iter \u003d %03i, acc \u003d %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n",
        "        print(msg)\n",
        "    num_iter +\u003d 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TST] iter \u003d 21000, acc \u003d 98.91 (2.71e+00s)\n"
          ]
        }
      ],
      "source": [
        "#Test code\n",
        "tic \u003d time.time()\n",
        "test_accuracy \u003d 0\n",
        "for begin in range(0, test_data.shape[0], batch_size):\n",
        "    end \u003d begin + batch_size\n",
        "    end \u003d min([end, test_data.shape[0]])\n",
        "            \n",
        "    batch_data \u003d np.zeros((end-begin, test_data.shape[1]))\n",
        "    batch_data \u003d test_data[begin:end,:]\n",
        "            \n",
        "    feed_dict \u003d {learning_obj.ph_data: batch_data, learning_obj.ph_dropout: 1}\n",
        "            \n",
        "    batch_labels \u003d np.zeros(batch_size)\n",
        "    batch_labels[:end-begin] \u003d test_labels[begin:end]\n",
        "    feed_dict[learning_obj.ph_labels] \u003d batch_labels\n",
        "            \n",
        "    batch_accuracy \u003d learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
        "    test_accuracy +\u003d batch_accuracy*batch_data.shape[0]\n",
        "test_accuracy \u003d test_accuracy/test_data.shape[0]\n",
        "test_time \u003d time.time() - tic\n",
        "msg \u003d \"[TST] iter \u003d %03i, acc \u003d %4.2f (%3.2es)\" % (num_iter, test_accuracy, test_time)\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "pycharm-56591012",
      "language": "python",
      "display_name": "PyCharm (GDL_HW1)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}