{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy.sparse.linalg as la\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as sio\n",
    "import scipy\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib inline\n",
    "\n",
    "import process_data\n",
    "\n",
    "from utils import layers\n",
    "from utils import process\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "\n",
    "#from models.base_gattn import BaseGAttN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT:\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "            bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(layers.attn_head(inputs, bias_mat=bias_mat,\n",
    "                out_sz=hid_units[0], activation=activation,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(layers.attn_head(h_1, bias_mat=bias_mat,\n",
    "                    out_sz=hid_units[i], activation=activation,\n",
    "                    in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(layers.attn_head(h_1, bias_mat=bias_mat,\n",
    "                out_sz=nb_classes, activation=lambda x: x,\n",
    "                in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "    \n",
    "        return logits\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.trainable_variables()\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "\n",
    "        # optimizer\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss+lossL2)\n",
    "        \n",
    "        return train_op\n",
    "\n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)\n",
    "        return tf.confusion_matrix(labels, preds)\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        loss=tf.reduce_mean(loss,axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main for gat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cora\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.001\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 2\n",
      "nb. units per layer: [256, 121]\n",
      "nb. attention heads: [4, 6]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x7fb9241e7d90>\n",
      "model: <class '__main__.GAT'>\n",
      "(2708, 2708)\n",
      "(2708, 1433)\n",
      "WARNING:tensorflow:From <ipython-input-3-7933bd74b040>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9b71373f15480589761678f428353a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop! Min loss:  0.6083112359046936 , Max accuracy:  0.8219999074935913\n",
      "Early stop model validation loss:  0.730624258518219 , accuracy:  0.8219999074935913\n",
      "INFO:tensorflow:Restoring parameters from pre_trained/cora/mod_cora.ckpt\n",
      "Test loss: 0.7084130644798279 ; Test accuracy: 0.8029999732971191\n"
     ]
    }
   ],
   "source": [
    "checkpt_file = 'pre_trained/cora/mod_cora.ckpt'\n",
    "\n",
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 1000 # ~3 min\n",
    "#nb_epochs = 100000 # OG epochs ~6h\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.001  # weight decay #\n",
    "#hid_units = [8] #OG # numbers of hidden units per each attention head in each layer \n",
    "hid_units = [256, 121]\n",
    "#n_heads = [8, 1] #OG # additional entry for the output layer\n",
    "n_heads = [4, 6] \n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = GAT\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = process.load_data(dataset)\n",
    "features, spars = process.preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "adj = adj.todense()\n",
    "\n",
    "features = features[np.newaxis]\n",
    "adj = adj[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "\n",
    "biases = process.adj_to_bias(adj, [nb_nodes], nhood=1)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n",
    "        bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n",
    "        lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n",
    "        msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n",
    "        attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        is_train = tf.placeholder(dtype=tf.bool, shape=())\n",
    "\n",
    "    logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n",
    "                                attn_drop, ffd_drop,\n",
    "                                bias_mat=bias_in,\n",
    "                                hid_units=hid_units, n_heads=n_heads,\n",
    "                                residual=residual, activation=nonlinearity)\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "    msk_resh = tf.reshape(msk_in, [-1])\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "\n",
    "    train_op = model.training(loss, lr, l2_coef)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "        tn= tnrange(nb_epochs)\n",
    "        for epoch in tn:\n",
    "            \n",
    "            tr_step = 0\n",
    "            tr_size = features.shape[0]\n",
    "\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        bias_in: biases[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                        is_train: True,\n",
    "                        attn_drop: 0.6, ffd_drop: 0.6})\n",
    "               \n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "                \n",
    "            vl_step = 0\n",
    "            vl_size = features.shape[0]\n",
    "\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        bias_in: biases[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                        is_train: False,\n",
    "                        attn_drop: 0.0, ffd_drop: 0.0})\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "            tn.set_description(\"v_a %s\"% str(val_acc_avg/vl_step))\n",
    "            #tn.update() \n",
    "            #print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "            #        (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "            #       val_loss_avg/vl_step, val_acc_avg/vl_step))\n",
    "\n",
    "            if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n",
    "                if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg/vl_step\n",
    "                    vlss_early_model = val_loss_avg/vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "\n",
    "        saver.restore(sess, checkpt_file)\n",
    "\n",
    "        ts_size = features.shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            loss_value_ts, acc_ts = sess.run([loss, accuracy],\n",
    "                feed_dict={\n",
    "                    ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    bias_in: biases[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                    is_train: False,\n",
    "                    attn_drop: 0.0, ffd_drop: 0.0})\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n",
    "\n",
    "        sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
